# 【《机器学习》第3章】线性模型

## 3.1 基本形式

线性模型(Linear Model)本质上就是一个试图学习出属性线性组合来进行预测的函数.一般向量形式为
$$f(\mathbf x)=\mathbf{w^Tx}+b$$
学得$\mathbf w$和$b$后,模型就能被确定.

## 3.2 线性回归

"线性回归"(linear regression)试图学得一个线性模型以尽可能准确地预测实值输出标记.一般使用最小二乘法找到一条直线,使得各个样本到直线上的距离之和最小.用矩阵形式表示为
$$\mathbf{\hat w}={\arg\min}_{\mathbf{\hat w}}\mathbf{(y-X\hat w)^T(y-X\hat w)}$$
当$\mathbf{X^TX}$为满秩矩阵或正定矩阵时,有闭式解
$$\mathbf{\hat w}^\ast=\mathbf{(X^TX)^{-1}X^Ty}$$
当$\mathbf{X^TX}$不是满秩矩阵或正定矩阵时,有多个解,选择哪个解作为输出由算法的归纳偏好确定,常见的做法是引入正则化(regularization)项.

同样也可以让线性模型逼近$y$的衍生物,如
$$\ln y=\mathbf{w^Tx}+b$$
这就是对数线性回归(log-linear regression).更一般的,考虑单调可微函数$g(\cdot)$,令
$$y=g^{-1}(\mathbf{w^Tx}+b)$$
这样得到的模型称为"广义线性模型"(generalized linear model).

## 3.3 对数几率回归

如果要做的是分类任务,只需要找一个单调可微函数将分类任务的真实标记$y$与线性回归模型的预测值联系起来.最理想的是"单位阶跃函数"(unit-step function),但是由于其不是连续的,所以需要找到一个"替代函数"(surrogate function).对数几率函数(logistic function)正是这样一个函数.
$$y=\frac{1}{1+e^{-(\mathbf{w^Tx}+b)}}$$
估计$\mathbf w,b$的过程见P59.

## 3.4 线性判别分析

线性判别分析(Linear Discriminant Analysis, LDA)的思想为:给定训练样例集,设法将样例投影到一条直线上,使得同类样例的投影点尽可能接近,异类样例投影点尽可能远离.

令$X_i,\mu_i,\Sigma_i$分别表示第$i$类示例的集合,均值向量,协方差矩阵,则可得到最大化的目标
$$J=\frac{\mathbf{w^T(\mu_0-\mu_1)(\mu_0-\mu_1)^Tw}}{\mathbf{w^T(\Sigma_0+\Sigma_1)w}}$$
定义类内散度矩阵(within-class scatter matrix)
$$S_w=\Sigma_0+\Sigma_1$$
以及类间散度矩阵(between-class scatter matrix)
$$S_b=\mathbf{(\mu_0-\mu_1)(\mu_0-\mu_1)^T}$$
求解过程见P61-62.

## 3.5 多分类学习

在实际中,有些二分类学习方法可以直接推广到多分类,但是在大部分情形下,我们利用二分类学习器来解决多分类问题.通常采用的方法是"拆解法",即将多分类任务拆解成二分类任务.常用的拆解策略有:

- 一对一(One vs. One, OvO):训练$N(N-1)/2$个分类器将$N$个类别两两分开;
- 一对其余(One vs. Rest, OvR):训练$N$个分类器,将每个样本与其余的样本分开;
- 多对多(Many vs. Many, MvM):常见的有"纠错输出编码"(Error Correcting Output Codes, ECOC)技术.见P65.

ECOC工作过程分为两步:编码和解码.在编码过程中,对$N$个类别做$M$次划分,每次划分将一部分类别划为正类,一部分划分为反类;解码过程中,使用$M$个分类器对测试样本进行预测,这些预测标记组成一个编码,然后将预测编码与每个类别各自的编码进行比较,返回距离最小的类别作为最终的预测结果.

ECOC在码长较小时可以根据这个原则计算出理论最优编码.然而码长较长时就难以有效地确定最优编码,这也是个NP难问题.

## 3.6 类别不平衡问题

类别不平衡(class-imbalance)就是指分类任务中不同类别的训练样例数目差别很大的情况.从线性分类器角度来看,用$y=\mathbf{w^Tx}+b$对样本$\mathbf x$进行分类时,实际上是用$y$与某一个阈值进行比较.通常用$0.5$作为阈值.但是由于我们通常假设训练集是真实样本总体的无偏采样,因此有如下的"再平衡"策略:

观测几率就代表了真实几率,设$m^+$和$m^-$分别为正例数目和反例数目,判别形式为
$$若\frac{y}{1-y}>\frac{m^+}{m^-}时,预测为正例$$

由于我们未必能基于训练集观测几率来推断出真实的几率,现在有三种做法:

- 直接对训练集里的反类样例进行欠采样(undersampling)
- 对训练集里的正类样例进行过采样(oversampling)
- 直接基于原始训练集进行学习,将上述再平衡策略纳入到决策过程中,称为"阈值移动"(hhreshold-moving).