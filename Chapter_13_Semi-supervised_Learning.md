# 【《机器学习》第13章】半监督学习

## 13.1 未标记样本

如果我们有训练样本集$D_l=\{(\mathbf x_1, y_1),(\mathbf x_2, y_2),\dots,(\mathbf x_l, y_l)\}$,这$l$个样本类别标记已知,则称为"有标记"(labeled)样本;此外还有$D_u=\{\mathbf x_{l+1}, \mathbf x_{l+2}, \dots, \mathbf x_{l+u}\},l\ll u$,这$u$个样本类别标记未知,称为"未标记"(unlabeled)样本.如果直接使用有标记样本,则$D_l$用来建模,$D_u$中的信息被浪费;另一方面,如果$D_l$较小,则训练样本不足,学得模型的泛化能力往往不佳.除了将$D_u$中样本全部进行标记外,还有几种代价较小的方法.

如果我们每次都能挑出对改善模型性能帮助较大的未标记样本,则能构建出较强的模型.这样的标记成本较小.这样的学习方式称为"主动学习"(active learning),其目的是通过使用尽可能少的"查询"(query)来获得尽量好的性能.

如果完全不使用查询这种主动干预的方式进行学习,只利用未标记样本的分布信息以及标记样本的信息进行学习,这样的方式仍然是可行的.

假设**未标记样本与有标记样本是从同样的数据源独立同分布采样而来**,则它们所包含的数据分布信息对于建模将会有很大帮助.这样就必然要做一些将未标记样本所揭示的数据分布信息与类别标记想联系的假设.常见的假设有"聚类假设"(cluster assumption)和"流形假设"(manifold assumption),这些假设的本质都是"相似的样本拥有相似的输出".

半监督学习可进一步划分为纯(pure)半监督学习和直推学习(transductive learning).纯半监督学习假定训练数据中的未标记样本并非待预测的数据,而直推学习假定学习过程中的未标记样本就是待预测数据.

## 13.2 生成式方法

生成式方法(generative methods)是直接基于生成式模型的方法.此类方法假设所有数据(无论是否有标记)都是由同一个潜在的模型"生成"的.生成式方法有好多种不同的类别,但是不同类别的主要区别在于生成式模型的假设.

以高斯混合模型为例,假设样本由$$p(\mathbf x)=\sum_{i=1}^N\alpha_i\cdot p(\mathbf x|\mathbf\mu_i,\mathbf\Sigma_i)$$生成,其中,$\alpha_i\ge 0,\sum_{i=1}^N\alpha_i=1$;$p(\mathbf x|\mathbf\mu_i,\mathbf\Sigma_i)$是样本$\mathbf x$属于第$i$个高斯混合成分的概率;$\mathbf\mu_i$和$\mathbf\Sigma_i$为该高斯混合成分的参数.

令$f(\mathbf x)\in\mathcal{Y}$表示模型$f$对$\mathbf x$的预测标记,$\Theta\in\{1,2,\dots,N\}$表示样本$\mathbf x$隶属的高斯混合成分.由最大化后验概率可知,
$$\begin{split}
f(\mathbf x)&={\arg\max}_{j\in\mathcal Y}p(y=j|\mathbf x)\\
&={\arg\max}_{j\in\mathcal Y}\sum_{i=1}^Np(y=j,\Theta=i|\mathbf x)\\
&={\arg\max}_{j\in\mathcal Y}\sum_{i=1}^Np(y=j|\Theta=i,\mathbf x)\cdot p(\Theta=i|\mathbf x)\\
\end{split}$$
其中,$p(\Theta=i|\mathbf x)=\frac{\alpha_i\cdot p(\mathbf x|\mu_i,\Sigma_i)}{\sum_{i=1}^N\alpha_i\cdot p(\mathbf x|\mu_i,\Sigma_i)}$为样本$\mathbf x$由第$i$个高斯混合成分生成的后验概率,$p(y=j|\Theta=i,\mathbf x)$为$\mathbf x$为$\mathbf x$由第$i$个高斯混合成分生成且其类别为$j$的概率.不难发现,$p(y=j|\Theta=i,\mathbf x)$该项需要知道样本的类别标记,而$p(\Theta=i|\mathbf x)$不需要知道样本的类别标记,通过引入大量的未标记数据,提升分类器的性能.

高斯混合模型参数可以通过EM算法求得.

将上述高斯混合模型替换成其他模型(如混合专家模型,朴素贝叶斯模型)即可得到其他半监督学习方法.但是需要注意的是,模型假设必须准确,否则利用未标记样本可能会降低学习器的泛化性能.

## 13.3 半监督SVM

半监督支持向量机(Semi-Supervised Support Vector Machine, S3VM)是支持向量机在半监督学习上的推广.在考虑未标记样本后,S3VM试图找到能将两类有标记样本区分开并且穿过数据低密度区域的划分超平面.

半监督支持向量机中最著名的是TSVM(Transductive Support Vector Machine).TSVM试图考虑对未标记样本进行各种可能的标记指派(label assignment),即尝试将每个未标记样本分别作为正样本或者负样本,然后在所有这些结果中,寻找一个在所有样本上间隔最大化的划分超平面.即
$$\begin{split}
\min_{w,b,\hat y,\xi}&\,\frac{1}{2}\|w\|_2^2+C_l\sum_{i=1}^l\xi_i+C_u\sum_{i=l+1}^m\xi_i\\
s.t. &y_i(w^T\mathbf x_i+b)\ge1-\xi_i,i=1,2,\dots,l,\\
&\hat y_i(w^T\mathbf x_i+b)\ge1-\xi_i,i=l+1,l+2,\dots,m,\\
&\xi_i\ge 0,i=1,2,\dots,m
\end{split}$$
其中$\xi_i$为松弛向量;$C_l$与$C_u$是由用户指定的用于平衡模型复杂度,有标记样本与未标记样本重要程度的折中参数.

但是,尝试所有未标记样本的标记指派是一个穷举过程,为提升算法效率,需要进行一定的优化:
1. 利用有标记样本学习得到一个SVM;
2. 利用这个SVM对未标记样本进行标记指派,需要注意的是,$C_u$要设置为比$C_l$小的值;
3. TSVM找出两个标记指派为异类并且很可能发生错误的未标记样本,交换其标记,更新划分超平面和松弛向量;
4. 重复上一步骤,并且在这个过程中逐渐增大$C_u$知道$C_l=C_u$为止.

如果在训练过程中出现类别不平衡问题,需要将优化目标中的$C_u$拆分为$C_u^+$和$C_u^-$两项,分别进行优化.

由以上可以看出,半监督SVM研究的一个重点是如何设计出高效的优化求解策略.

## 13.4 图半监督学习

给定一个数据集,我们将其映射为一个图,数据集中每个样本对应于图中一个结点,若两个样本间的相似度很高,则对应的结点之间存在一条边,边的"强度"(strength)正比于样本之间的相似度(或相关性).*通过建立起相似度矩阵,我们利用已标记样本的信息对未标记样本进行标记指派,在得到所有或者大部分未标记样本的标记后,再利用有监督学习的方法进行学习.*

首先基于$D_l\cup D_u$构建一个图$G=(V,E)$,边集$E$可表示为一个亲和矩阵(affinity matrix):
$$\begin{split}
(\mathbf W)_{ij}=\begin{cases}
\exp(-\frac{\|\mathbf x_i-\mathbf x_j\|_2^2}{2\sigma^2}),&\text{if}\;i\ne j\\
0,&\text{otherwise}
\end{cases}\end{split}$$
其中$i,j\in\{1,2,\dots,m\}$,$\sigma>0$是用户指定的高斯函数带宽参数.

加入从图$G$学得一个实值函数$f:V\to\mathbb R$,则可定义关于$f$的"能量函数"(energy function):
$$
E(f)=\frac{1}{2}\sum_{i=1}^m\sum_{i=1}^m(\mathbf W)_{ij}(f(\mathbf x_i)-f(\mathbf x_j))^2
$$
具有最小能量的函数$f$在有标记样本上满足$f(\mathbf x_i)=y_i(i=1,2,\dots,l)$,在未标记样本上满足$\mathbf\Delta\mathbf f=0$,其中,$\mathbf\Delta=\mathbf D-\mathbf W$为拉普拉斯矩阵.

在此定义下,可以推导得到二分类问题和多类问题的标记传播方法(见P302-303).同时可以看出,此类算法的缺陷很明显:存储开销为$O(m^2)$,无法处理大规模数据;构图过程仅能考虑训练样本集,难以判断新样本在图中的位置.

## 13.5 基于分歧的方法

基于分歧的方法(disagreement-based methods)使用多学习器,而学习器之间的"分歧"(disagreement)对未标记数据的利用至关重要.

"协同训练"(co-training)是此类方法的重要代表.它最初是针对"多视图"(multi-view)数据设计的,因此也被看做"多视图学习"(multi-view learning)的代表.一个数据对象往往拥有多个"属性集"(attribute set),每个属性集就构成了一个"视图"(view).假设不同视图具有"相容性"(compatibility),即其所包含的关于输出空间$\mathcal Y$的信息是一致的.在此假设下,显式地考虑多视图有很多好处.在相容性基础上,不同视图信息的"互补性"会给机器学习的构建带来很多便利.

协同训练正是很好地利用了多视图的"相容互补性".其利用未标记数据的步骤为:
1. 在每个视图上基于有标记样本分别训练出一个分类器;
2. 让每个分类器分别挑选出置信度较高的未标记样本进行标记指派;
3. 将上一步中进行标记指派的样本提供给另外一个分类器进行训练更新,再将后一分类器中标记指派的样本及其指派标记输送回前一分类器进行训练更新;
4. 不断重复前两步,直至算法收敛.

视图的条件独立在现实任务中很难被满足,因此提升的幅度不会那么大.但即便是在更弱的条件下,协同训练仍可有效地提升若分类器的性能.

为了使用此方法,需能生成具有显著分歧,性能尚可的多个学习器,但是当有标记样本很少,尤其是数据不具有多个视图时,要做到这一点并不容易,需有巧妙地设计.

## 13.6 半监督聚类

聚类是一种无监督学习,在实际中我们通常可以获得一些额外的监督信息,因此可以通过半监督聚类(semi-supervised clustering)来利用监督信息以获得更好的聚类效果.

聚类任务中获得的监督信息可以分为两种:

1. "必连"与"勿连"信息;
2. 少量有标记样本.

对于第一种使用约束$k$均值聚类算法进行聚类;对于第二类,直接将它们作为"种子",用它们初始化$k$均值算法的$k$个聚类中心,并且在聚类簇迭代过程中不改变种子样本的簇隶属关系.
