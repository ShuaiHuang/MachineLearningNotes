\chapter{计算学习理论}
% # 【《机器学习》第12章】计算学习理论

本章主要从"计算"的角度来分析机器学习算法,并为机器学习算法提供理论支撑.由于牵涉到大量的概念定义,本章的笔记不做过多重复,仅仅从思路上对各个概念做分析串联.

\section{基础知识}
% ## 12.1 基础知识

本节首先重新强调了泛化误差与经验误差这两个概念.这两个概念在本章第2章中就已经被引入过.大致可以理解为,泛化误差是训练出的模型在除训练样本外的新样本集上的误差;经验误差是训练出的模型在训练集上的误差.本章后续的部分重点关注经验误差与泛化误差之间的逼近程度.

在本章后面的部分,经常要用到下列几个不等式:
\begin{itemize}
\item Jensen不等式
\item Hoeffding不等式
\item McDiarmid不等式
\end{itemize}

\section{PAC学习}
% ## 12.2 PAC学习

概念这个定义可以理解为,从样本空间到标记空间的映射,而我们所关注的本质上的规律为目标概念,所有目标概念的集合为概念类.从学习算法较多来看,他所考虑的所有可能概念的集合称为假设空间.通常情况下,假设空间和概念空间通常是不同的.若目标概念被包含在假设空间中,则称该假设空间对应的学习算法是"可分的"(separable)或者"一致的"(consistent).反之,则称学习算法是"不可分的"(non-separable)或者"不一致的"(non-consistent).

我们通常所期望的是通过学习算法学得的假设尽可能接近目标概念.但是由于受到训练数据集采样的因素制约,可能会存在若干个等效的假设,如何从这些等效假设中以较大的概率选取出最接近目标概念的假设,是PAC学习的目的.

PAC学习包含:PAC辨识,PAC可学习,PAC学习算法,样本复杂度等定义.如果在PAC学习中假设空间和概念类完全相同,则称"恰PAC可学习"(properly PAC learnable).但是实际情况中研究的重点是假设空间与概念类不同的情况,一般而言,假设空间越大,包含目标概念的可能性越大,但从中找到目标概念的难度也就越大.当假设空间规模有限时,则称为有限假设空间,否则称为无限假设空间.

\section{有限假设空间}
% ## 12.3 有限假设空间

\subsection{可分情形}
% ### 12.3.1 可分情形

可分情形是目标概念包含在假设空间中的情况,如果训练集足够大,则可通过不断去除假设集合中不满足训练集样本标记的假设,直到假设集合中仅剩下一个集合为止.但是如果假设空间中剩余多个假设时,就无法区分剩余假设的优劣了.

从另外一个角度看,到底需要多少样本才能区分学得目标概念的有效近似呢?对于PAC学习来说,只要训练集合$D$的规模能使学习算法$\mathcal L$能以概率$1-\delta$找到目标概念的$\epsilon$近似即可.

\begin{enumerate}
\item 首先估计泛化误差大于$\epsilon$但是对于分布$\mathcal D$上任何样例$(\mathbf x, y)$有 
\begin{equation}
\begin{split}P\Big(h(\mathbf x)=y\Big)&=1-P\Big(h(\mathbf x)\ne y\Big) \\&=1-E(h)\\&<1-\epsilon\end{split}
\end{equation}
\item 同样有经验误差 
\begin{equation}
\begin{split}P\Big((h(\mathbf x_1)=y_1)\land(h(\mathbf x_2)=y_2)\land\dots\land(h(\mathbf x_m)=y_m)\Big)&=(1-P(h(\mathbf x)\ne y)^m)\\&<(1-\epsilon)^m\end{split}
\end{equation}
\item 泛化误差大于$\epsilon$且在训练集上表现完美的所有假设出现概率不大于$\delta$ 
\begin{equation}
\begin{split}P\Big(h\in\mathcal H:E(h>\epsilon)\land\widehat E(h)=0\Big)&<|\mathcal H|(1-\epsilon)^m\\&<|\mathcal H|e^{-m\epsilon}\end{split}
\end{equation}
\item 令上式不大于$\delta$,即可导出
\begin{equation}
m\ge \frac{1}{\epsilon}(\ln|\mathcal H|+\ln\frac{1}{\delta})
\end{equation}
\end{enumerate}

从以上可以看出,所有有限假设空间都是PAC可学习的.

但是大部分模型的参数选择范围都在无限区间上,即假设空间是无限的.是否可对参数选择范围作出限制将其转化为有限假设空间?(具体可看12.4节.)

\subsection{不可分情形}
% ### 12.3.2 不可分情形

对于较难的学习问题,或者说大部分情形来说,目标概念往往不在假设空间中.这时可以衡量经验误差$\widehat E(h)$与泛化误差$E(h)$之间的差异,有
\begin{equation}
P\Big(|\widehat E(h)-E(h)|\ge\epsilon\Big)\le2\exp\Big(-2m\epsilon^2\Big)
\end{equation}
.

在此情况下,可以从假设空间中找出一个泛化误差最小的假设,这个$\epsilon$近似也是一个比较好的学习目标.基于此可以引出不可知学习(agnostic PAC learnable)的定义.

\section{VC维}
% ## 12.4 VC维

现实学习任务所面临的通常是假设空间,欲对此种情形的可学习情况进行研究,需度量假设空间的复杂度.

增长函数$\Pi_\mathcal H(m)$表示假设空间$\mathcal H$对$m$个示例所能赋予标记的最大可能结果数.对二分类问题来说,$\mathcal H$中的假设对$D$中示例赋予标记的每种可能结果称为对$D$的一种"对分".若假设空间$\mathcal H$能实现示例集$D$上的所有对分,即$\Pi_\mathcal H(m)=2^m$,则称示例集$D$能被假设空间$\mathcal H$"打散".

假设空间$\mathcal H$的VC维是能被$\mathcal H$打散的最大示例集的大小.(结合例12.1和例12.2,首先从示例集入手,逐渐增加示例集中样本的个数,进而确定VC维的大小.)

基于VC维的泛化误差界是分布无关,数据独立的.任何VC维有限的假设空间$\mathcal H$都是(不可知)PAC可学习的.

\section{Rademacher复杂度}
% ## 12.5 Rademacher复杂度

由于VC维泛化误差界是分布无关,数据独立的,因此具有"普适性".但是从另一方面看,这样的泛化误差界通常也比较"松".Rademacher复杂度(Rademacher complexity)是另一种刻画假设空间复杂度的途径,它在一定程度上考虑了数据分布.

为了计算泛化误差界,本节首先定义了经验Radematcher复杂度和Rademacher复杂度.最后计算出针对回归问题和二分类问题的泛化误差界.这些Rademacher的泛化误差界与分布$\mathcal D$或数据$D$有关.

\section{稳定性}
% ## 12.6 稳定性

无论是基于VC维或者是Rademacher复杂度来推导泛化误差界,所得到的结果均与具体学习算法无关,所所有的学习算法均适用.稳定性分析是针对学习算法考察的一种重要方式.算法的稳定性是指算法在输入发生变化时,输出是否会随之发生较大的变化.

输入变化即指样例集变化:移除第$i$个样例得到的集合;替换第$i$个样本得到的集合.

损失函数即预测标记与真实标记之间的差别:泛化损失;经验损失;留一损失.

定义12.10定义了$\mathcal L$关于损失函数$\mathcal l$的$\beta$-均匀稳定性,需要注意的是,移除示例的稳定性包含替换示例的稳定性.

学习算法的稳定性分析关注的是$|\hat l(\mathcal L,D)-l(\mathcal L,D)|$,而假设空间复杂度分析关注的是$\sup_{h\in\mathcal H}(\widehat E(h)-E(h))$.从以上可以看出,稳定性假设不用分析所有可能的假设,只需要根据算法本身的特性来讨论输出假设的泛化误差界.

定理12.9表明了学习算法稳定性与假设空间复杂度之间的关系.学习算法的稳定性和假设空间的复杂度并非无关,由稳定性的定义可知,两者通过损失函数$\mathcal l$联系起来.