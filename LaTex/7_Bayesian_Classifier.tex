\chapter{贝叶斯分类器}
% # 【《机器学习》第7章】贝叶斯分类器

% 工作之后每天学习时间缩短了不少，为了保证学习质量，每天学习完相应的章节之后，对当天所学的内容整理出脉络，形成反馈，保证学习质量。学习的教材为周志华所著的《机器学习》。

\section{贝叶斯判别核心}
% ## 7.1 贝叶斯判别核心

- 最小化错误率贝叶斯分类器
\begin{equation}
h^\ast=argmax_{c\in \mathcal{y}}P(c|\mathbf{x})
\end{equation}

\begin{itemize}
\item 后验概率两种不同的生成方式
    \begin{itemize}
    \item 判别式模型 discriminative model 直接生成$P(c|\mathbf{x})$
    \item 生成式模型 generative model 先生成$P(\mathbf{x}, c)$再由之推导出$P(c|\mathbf{x})$
    \end{itemize}
\item 进一步分解
    \begin{equation}
    P(c|\mathbf{x})=\frac{P(\mathbf{x}, c)}{P(\mathbf{x})}=\frac{P(\mathbf{x}|c)P(c)}{P(\mathbf{x})}
    \end{equation}
    \item 其中，$P(\mathbf{x})$和$P(c)$可以从抽样样本中估计得出
    \item 问题是如何得出$P(\mathbf{x}|c)$，进而推导出后验概率$P(c|\mathbf{x})$
\end{itemize}

\section{极大似然估计（频率主义学派）}
% ## 7.2 极大似然估计（频率主义学派）

\subsection{频率主义学派与贝叶斯学派之争}
% ### 频率主义学派与贝叶斯学派之争

\begin{itemize}
\item 频率主义学派观点：模型参数是一个客观固定值
\item 贝叶斯学派观点：模型参数是一个未观察到的随机变量，服从一定的分布规律
\item 因此，求取$P(\mathbf{x}|c)$的过程进一步被归结为求取$P(\mathbf{x}|\mathbf{\theta}_c)$的过程
\end{itemize}

\subsection{极大似然估计}
% ### 极大似然估计

\begin{itemize}
\item 最优模型参数$\mathbf{\theta_c}$一定是能够让当前观察值以最大概率出现的参数
\item 似然定义$P(\mathcal{D_c}|\mathbf{\theta_c})=\prod_{\mathbf{x}\in\mathcal{D}_c}P(\mathbf{x}|\mathbf{\theta_c})$
\item 防止下溢，取对数$\log P(\mathcal{D_c}|\mathbf{\theta_c})=\sum_{\mathbf{x}\in\mathcal{D}_c}\log P(\mathbf{x}|\mathbf{\theta_c})$
\item 极大似然估计缺点模型必须准确，如果模型不准确，即使参数准确，也有可能出现偏差。即数据潜在分布规律必须要与估计分布相一致
\end{itemize}

\subsection{朴素贝叶斯模型（贝叶斯学派）}
% ## 7.3 朴素贝叶斯模型（贝叶斯学派）

\begin{itemize}
\item $P(\mathbf{x}|c)$是所有属性值上的联合分布，对其进行估计存在一定的困难
\item 引入前提假设：所有的属性值$x_i$相互独立$$P(c|\mathbf{x})=\frac{P(\mathbf{x}|c)P(c)}{P(\mathbf{x})}=\frac{P(c)\prod_{i=1}^dP(x_i|c)}{P(\mathbf{x})}$$
\item 由样本集$\mathcal{D}$得出：每个类别$c$上属性值$x_i$的条件概率$P(x_i|c)$
\item 为防止某个类别上属性值样本数为0，对样本进行平滑，常用拉普拉斯修正
    \begin{itemize}
    \item $\widehat{P}(c)=\frac{|D_c|+1}{|D|+N}$
    \item $\widehat{P}(x_i|c)=\frac{|D_{c_ix_i}|+1}{|D_c|+N_i}$
    \item 拉普拉斯修正在样本数量较少时避免了因为某些属性上样本数量为0而产生估计偏差；在样本量较大时，引入先验的影响可以忽略不计
    \end{itemize}
\end{itemize}

\section{半朴素贝叶斯分类器}
% ## 7.4 半朴素贝叶斯分类器

\begin{itemize}
\item 朴素贝叶斯分类器的前提条件为：所有属性之间相互独立。然而这一条件在实际中很难被满足，由此，半朴素贝叶斯分类器（semi-na$\ddot{i}$ve bayesian classifiers）被提出，用以改进朴素贝叶斯前提条件中的限制性因素。
\item One Dependent Estimator(ODE)即独依赖估计是半朴素贝叶斯分类器的一个常用策略。独依赖估计是指没一个因素在类别之外至多仅依赖一项其他因素。
\begin{equation}
P(c|\mathbf{x})\propto P(c)\prod_{i=1}^dP(x_i|c,pa_i)
\end{equation}
\item Super Parent ODE(SPODE)即超父独依赖估计假定所有的属性都依赖于同一个父属性
\item Tree Augmented naive bayesian(TAN)通过最大带权生成树算法的基础上，通过以下步骤对属性间依赖关系进行简化
    \begin{enumerate}
    \item 计算两两属性间的条件互信息
    \begin{equation}I(x_i,x_j|y)=\sum_{x_i,x_j,c\in\mathcal{y}}P(x_i,x_j|c)\log\frac{P(x_i,x_j|c)}{P(x_i|c)P(x_j|c)}
    \end{equation}
    \item 生成全连接图，以互信息为两两结点间的权重
    \item 由此构建最大带权生成树需要进一步学习，挑选根向量，将边置为有向
    \item 增加类别节点$y$，增加从类别节点到各个属性节点的有向边
\end{enumerate}
\item Averaged ODE(AODE)是一种基于集成学习机制，更为强大的独依赖分类器
    \begin{enumerate}
    \item AODE尝试将每一个节点定义成父节点，构建SPODE
    \item 将那些有足够的数据支撑的SPODE集成起来作为最终结果
    \begin{equation}
    P(c|\mathbf{x})\propto \sum_{i=1, |D_{x_i}|\ge m'}^dP(c,x_i)\prod_{j=1}^dP(x_j|c,x_i)
    \end{equation}
    \item AODE无需模型选择
    \end{enumerate}
\item 如果引入高阶属性依赖？
    \begin{itemize}
    \item 如果训练样本足够多，则泛化能力可以增强
    \item 如果训练样本不足，则陷入高阶联合概率的泥沼
    \end{itemize}
\end{itemize}

\section{贝叶斯网}
% ## 7.5 贝叶斯网

\begin{itemize}
\item 贝叶斯网亦称信念网(belief network)，使用有向非环图（Directed Acyclic Graph）来说明各个属性之间的依赖关系。
\item 有向非环图由结构$G$和参数$\mathcal{\Theta}$两部分构成，即
\begin{equation}
G=<B|\Theta>
\end{equation}
\end{itemize}

\subsection{结构}
% ### 7.5.1 结构

\begin{itemize}
\item 贝叶斯网有效表达出了属性间的条件独立性（给定父节点集，每个节点与其非后裔节点相互独立）
\begin{equation}
P_B(x_1,x_2,\dots,x_n)=\prod_{i=1}^{d}P_B(x_i|\pi_i)=\prod_{i=1}^d\theta_{x_i|\pi_i}
\end{equation}
\item 常见的网络结构有
    \begin{itemize}
    \item 同父结构
    \item V型结构
    \item 顺序结构
    \end{itemize}
\item V型结构又称冲撞结构，具有边际条件独立性。
    \begin{itemize}
    \item 当$x_1$同时依赖于$x_2,x_3$时，若$x_1$值固定，则$x_2$与$x_3$不相互独立，若$x_1$值不固定，则$x_2$与$x_3$相互独立
    \item 证明过程\begin{equation}P(x_2,x_3)=\sum_{x_1}P(x_2,x_3,x_1)=\sum_{x_1}P(x_1|x_2,x_3)P(x_2)P(x_3)=P(x_2)P(x_3)\end{equation}
    \end{itemize}
\item 条件独立性判别方式：
    \begin{enumerate}
    \item 生成道德图
        \begin{enumerate}
        \item 将所有V型结构父节点用无向边进行连接
        \item 将所有有向边变成无向边
        \end{enumerate}
    \item 给定属性$x,y$和属性集$\mathbf{z}$，若在属性集中去除$\mathbf{z}$属性后，$x,y$节点没有通路进行连接，则$x,y$关于属性集$\mathbf{z}$条件独立。
    \end{enumerate}
\end{itemize}

\subsection{学习}
% ### 7.5.2 学习

\begin{itemize}
\item 学习过程包括学习贝叶斯网络结构$B$和网络参数$\Theta$，在$B$确定的情况下，$\Theta$可以通过计算样本频率进行确定。所以首要目的是学习网络结构$B$。
\item 评分搜索是最常用的方法。通过评分函数(core function)来评价网络结构和数据集的契合程度，并且其中包含着归纳偏好。
\item 学习准则最小描述长度(Minimal Description Length)。找到一个最短编码长度来对模型和数据集中的数据分布来进行描述。其中包含了描述该模型自身所需要的字节长度和使用该模型描述数据所需要的字节长度。即\begin{equation}s(B|D)=f(\theta)|B|-LL(B|D)\end{equation}，其中，$f(\theta)$是描述每个属性所用的字节长度，|B|是网络结构中包含的属性数目，\begin{equation}LL(B|D)=\sum_{i=1}^m\log P_B(x_i)\end{equation}
    \begin{itemize}
    \item $f(\theta)=1$时，为Akaike Information Criterion评分函数
    \item $f(\theta)=\frac{1}{2}\log m$时，为Bayesian Information Criterion评分函数
    \item $f(\theta)=0$时，退化为极大似然估计
    \end{itemize}
\item 从所有可能的网络结构中进行遍历搜索得到最优网络结构是一个NP难问题，一般通过启发式搜索得到次优网络结构
    \begin{itemize}
    \item 贪心法从某个网络结构出发，每次调整一条边，直到评分函数稳定为止
    \item 通过给网络结构施加约束来削减搜索空间
    \end{itemize}
\end{itemize}

\subsection{推断}
% ### 7.5.3 推断

\begin{itemize}
\item 在得出网络结构之后，最精确的做法是求出联合概率分布，进而得出后验概率。但是在网络结构比较复杂的情况下，计算量非常大，不容易实现。
\item 这种情况下就要使用近似推断在有限时间内取得近似解。
\item 吉布斯采样(Gibbs sampling)原理：
    \begin{itemize}
    \item 假设$\mathbf{Q}=\{Q_1,Q_2,\dots,Q_n\}$是待查询变量，$\mathbf{E}=\{E_1,E_2,\dots,E_k\}$是证据变量，目标是计算后验概率$P(\mathbf{Q}|\mathbf{E})$。
    \item 随机产生一个符合$\mathbf{E=e}$的样本作为初始起点
    \item 对非证据变量进行逐个采样改变其取值，采样概率根据贝叶斯网和其他变量的当前取值计算获得。
    \item 假定经过$T$次采样得到的与$\mathbf{q}$一致的样本有$n_q$个，则可近似估算出后验概率\begin{equation}P(\mathbf{Q}|\mathbf{E})\simeq \frac{n_q}{T}\end{equation}
    \end{itemize}
\item 吉布斯采样实质上是在$\mathbf{E=e}$的子空间上进行的随机漫步，每一步依赖于前一步的状态，所以这是一个马尔科夫链
    \item 需要较长时间才能收敛
    \item 如果网络中存在有0或1这样的极端概率，则可能收敛于一个错误的状态
\end{itemize}

\section{EM算法}
% ## 7.6 EM算法

\begin{itemize}
\item 在某些变量未被观测到的情况下，直接做最大似然估计，则需要做边际最大似然\begin{equation}L(\mathbf{\Theta|X})=\ln P(\mathbf{X|\Theta})=\ln \sum_{\mathbf{Z}}P(\mathbf{X,Z}|\Theta)\end{equation}
\item EM算法原型
    \begin{itemize}
    \item E步 求出关于$P(\mathbf{Z}|\mathbf{X},\Theta^t)$的分布，并计算对数似然关于$\mathbf{Z}$的期望
    \item M步 寻找最大化期望似然
    \end{itemize}
\item 隐变量估计也可以通过梯度下降法求解，但是求和项数将会随着隐变量的数目以指数级上升
\item EM算法是一种非梯度求解方法
\end{itemize}
