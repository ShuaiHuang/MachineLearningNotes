\chapter{绪论}

\section{引言}

机器学习致力于研究如何通过计算的手段,利用经验来改善系统自身的性能.

\section{基本术语}

本节用西瓜举了一些机器学习相关的基本例子,引入一些机器学习的术语,在此不做展开.

\section{假设空间}

归纳(induction)是从特殊到一般的"泛化"(generalize)过程,即从具体的事实归结出一般规律;"演绎"(deduction)则是从一般到特殊的"特化"(specialization)过程,即从基础原理推演出具体情况.从样例中学习显然是一个归纳过程,因此亦称"归纳学习"(inductive learning).

学习过程实质上就是在所有假设组成的空间中进行搜索的过程.可能有多个假设与训练集一致,这些假设的集合称之为"版本空间".

\section{归纳偏好}

归纳偏好是本章的重要内容,也是机器学习能够解决实际问题的理论基础.

在上一节版本空间的举例中,有三种假设满足判定条件,但是无法进一步判定这三个假设哪一个更好.但是对于一个具体的机器学习模型而言,它必须要产生一个模型,此时学习算法本身的归纳偏好就会起到重要作用.任何一个有效的机器学习算法必有其归纳偏好,否则就会被版本空间中的等效假设所迷惑,无法产生确定的学习结果.

归纳偏好可看作学习算法在庞大的假设空间中对假设所进行选择的启发式或价值观."奥卡姆剃刀"(Occam's razor)是一种一般性的原则来引导建立"正确的"偏好,其内容为"若有多个假设与观察一致,则选择最简单的那个".归纳偏好的原则也不能一概而论,算法的归纳偏好是否与问题本身匹配,大多数时候决定了算法能否取得好的性能.

假设$P(h|\mathbf X,\mathcal L_a)$代表算法$\mathcal L_a$基于训练数据$\mathbf X$产生假设$h$的概率,再令$f$代表希望学习的真实函数.$\mathcal L_a$的"训练集外误差"为
\begin{equation}
E_{ote}(\mathcal L_a|\mathbf X, f)=\sum_h\sum_{x\in\mathcal X-x}P(x)\text{I}(h(x)\ne f(x))P(h|\mathbf X,\mathcal L_a)
\end{equation}
其中$\text{I}(\cdot)$是指示函数.

考虑二分类问题,对所有可能的$f$按均匀分布对误差求和,有
\begin{equation}
\sum_fE_{ote}(\mathcal L_a|\mathbf X,f)=2^{|\mathcal X|-1}\sum_{x\in\mathcal X-x}P(x)\cdot 1
\end{equation}
误差与学习算法无关,这就是"没有免费午餐"(No Free Lunch Theorem, NFL定理).

但是需要注意的是,NFL引入了一个重要前提,所有"问题"出现的机会相同,或所有问题同等重要,即所有可能的$f$按均匀分布.因此必须清楚地认识到,脱离具体问题空谈"什么算法更好"毫无意义.学习算法自身的归纳偏好与问题是否相匹配,起到决定性作用.