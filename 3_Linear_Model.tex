\chapter{线性模型}

\section{基本形式}
% ## 3.1 基本形式

线性模型(Linear Model)本质上就是一个试图学习出属性线性组合来进行预测的函数.一般向量形式为
\begin{equation}
f(\mathbf x)=\mathbf{w^Tx}+b
\end{equation}
学得$\mathbf w$和$b$后,模型就能被确定.

\section{线性回归}
% ## 3.2 线性回归

``线性回归''(linear regression)试图学得一个线性模型以尽可能准确地预测实值输出标记.一般使用最小二乘法找到一条直线,使得各个样本到直线上的距离之和最小.用矩阵形式表示为
\begin{equation}
\mathbf{\hat w}={\arg\min}_{\mathbf{\hat w}}\mathbf{(y-X\hat w)^T(y-X\hat w)}
\end{equation}
当$\mathbf{X^TX}$为满秩矩阵或正定矩阵时,有闭式解
\begin{equation}
\mathbf{\hat w}^\ast=\mathbf{(X^TX)^{-1}X^Ty}
\end{equation}
当$\mathbf{X^TX}$不是满秩矩阵或正定矩阵时,有多个解,选择哪个解作为输出由算法的归纳偏好确定,常见的做法是引入正则化(regularization)项.

同样也可以让线性模型逼近$y$的衍生物,如
\begin{equation}
\ln y=\mathbf{w^Tx}+b
\end{equation}
这就是对数线性回归(log-linear regression).更一般的,考虑单调可微函数$g(\cdot)$,令
\begin{equation}
y=g^{-1}(\mathbf{w^Tx}+b)
\end{equation}
这样得到的模型称为``广义线性模型''(generalized linear model).

\section{对数几率回归}
% ## 3.3 对数几率回归

如果要做的是分类任务,只需要找一个单调可微函数将分类任务的真实标记$y$与线性回归模型的预测值联系起来.最理想的是``单位阶跃函数''(unit-step function),但是由于其不是连续的,所以需要找到一个``替代函数''(surrogate function).对数几率函数(logistic function)正是这样一个函数.
\begin{equation}
y=\frac{1}{1+e^{-(\mathbf{w^Tx}+b)}}
\end{equation}
估计$\mathbf w,b$的过程见P59.

\section{线性判别分析}
% ## 3.4 线性判别分析

线性判别分析(Linear Discriminant Analysis, LDA)的思想为:给定训练样例集,设法将样例投影到一条直线上,使得同类样例的投影点尽可能接近,异类样例投影点尽可能远离.

令$X_i,\mu_i,\Sigma_i$分别表示第$i$类示例的集合,均值向量,协方差矩阵.若将数据投影到直线$\mathbf w$上,则两类样本的中心在直线上的投影分别为$\mathbf{w^T\mu_0}$和$\mathbf{w^T\mu_1}$;若将所有样本点都投影到直线上,则两类样本的协方差分别为$\mathbf{w^T\Sigma_0w}$和$\mathbf{w^T\Sigma_1w}$.则可得到最大化的目标
\begin{equation}\begin{split}
J&=\frac{\|\mathbf{w^T\mu_0-w^T\mu_1}\|_2^2}{\mathbf{w^T\Sigma_0w+w^T\Sigma_1w}}\\
&=\frac{\mathbf{w^T(\mu_0-\mu_1)(\mu_0-\mu_1)^Tw}}{\mathbf{w^T(\Sigma_0+\Sigma_1)w}}
\end{split}\end{equation}
定义类内散度矩阵(within-class scatter matrix)
\begin{equation}
S_w=\Sigma_0+\Sigma_1
\end{equation}
以及类间散度矩阵(between-class scatter matrix)
\begin{equation}
S_b=\mathbf{(\mu_0-\mu_1)(\mu_0-\mu_1)^T}
\end{equation}
求解过程见P61-62.

\section{多分类学习}
% ## 3.5 多分类学习

在实际中,有些二分类学习方法可以直接推广到多分类,但是在大部分情形下,我们利用二分类学习器来解决多分类问题.通常采用的方法是``拆解法'',即将多分类任务拆解成二分类任务.常用的拆解策略有:
\begin{itemize}
\item 一对一(One vs. One, OvO):训练$N(N-1)/2$个分类器将$N$个类别两两分开;
\item 一对其余(One vs. Rest, OvR):训练$N$个分类器,将每个样本与其余的样本分开;
\item 多对多(Many vs. Many, MvM):常见的有``纠错输出编码''(Error Correcting Output Codes, ECOC)技术.见P65.
\end{itemize}

ECOC工作过程分为两步:编码和解码.在编码过程中,对$N$个类别做$M$次划分,每次划分将一部分类别划为正类,一部分划分为反类;解码过程中,使用$M$个分类器对测试样本进行预测,这些预测标记组成一个编码,然后将预测编码与每个类别各自的编码进行比较,返回距离最小的类别作为最终的预测结果.

ECOC在码长较小时可以根据这个原则计算出理论最优编码.然而码长较长时就难以有效地确定最优编码,这也是个NP难问题.

\section{类别不平衡问题}
% ## 3.6 类别不平衡问题

类别不平衡(class-imbalance)就是指分类任务中不同类别的训练样例数目差别很大的情况.从线性分类器角度来看,用$y=\mathbf{w^Tx}+b$对样本$\mathbf x$进行分类时,实际上是用$y$与某一个阈值进行比较.通常用$0.5$作为阈值.但是由于我们通常假设训练集是真实样本总体的无偏采样,因此有如下的``再平衡''策略:

观测几率就代表了真实几率,设$m^+$和$m^-$分别为正例数目和反例数目,判别形式为
\begin{equation}\label{eq:rescaling}
\text{若}\frac{y}{1-y}>\frac{m^+}{m^-}\text{时,预测为正例}
\end{equation}

由于我们未必能基于训练集观测几率来推断出真实的几率,现在有三种做法:
\begin{itemize}
\item 直接对训练集里的反类样例进行欠采样(undersampling)
\item 对训练集里的正类样例进行过采样(oversampling)
\item 直接基于原始训练集进行学习,将式(\ref{eq:rescaling})所代表的再平衡策略纳入到决策过程中,称为``阈值移动''(shreshold-moving).
\end{itemize}