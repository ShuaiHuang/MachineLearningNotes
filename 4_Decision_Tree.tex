\chapter{决策树}

\section{基本流程}
% ## 4.1 基本流程

决策树(decision tree)生成过程是一个递归过程,即从当前结点寻找最优属性,根据属性的不同取值对当前节点对应的样本集进行划分.有三种情形会导致递归返回:

\begin{itemize}
\item 当前结点包含的样本全属于同一个类别,无需划分;
\item 当前属性集为空,或是所有样本在所有属性上取值相同,无法划分;
\item 当前结点包含的样本集合为空,不能划分.
\end{itemize}

\section{划分选择}
% ## 4.2 划分选择

决策树生成过程中最重要的一个环节是如何选取最优属性在当前结点对应的样本集上进行划分,我们希望决策树的分支节点所包含的样本尽可能属于同一个类别,即节点的"纯度"(purity)越来越高.

\subsection{信息增益}
% ### 4.2.1 信息增益

"信息熵"(information entropy)是度量样本集合纯度最常用的一种指标.设当前样本集合$\mathcal D$中第$k$类样本所占的比例为$p_k(k=1,2,\dots,|\mathcal Y|)$,则$D$的信息熵定义为
\begin{equation}
\text{Ent}(D)=-\sum_{k=1}^{|\mathcal Y|}p_k\log_2p_k
\end{equation}
$\text{Ent}(D)$的值越小,$D$的纯度越高.

设$D^v$表示$D$中所有在属性$a$上取值为$v$的样本,记为$D^v$,则有"信息增益"(information gain)
\begin{equation}
\text{Gain}(D,a)=\text{Ent}(D)-\sum_{v=1}^V\frac{|D^v|}{|D|}\text{Ent}(D^v)
\end{equation}

信息增益对取值数目较多的属性有偏好,通常使用"增益率"(gain ratio)来选择最优划分属性.
\begin{equation}
\text{Gain\_ratio}(D,a)=\frac{\text{Gain}(D,a)}{\text{IV(a)}}
\end{equation}
其中,
\begin{equation}
\text{IV}(a)=-\sum_{v=1}^V\frac{|D^v|}{|D|}\log_2\frac{|D^v|}{|D|}
\end{equation}
称为属性$a$的"固有值"(intrinsic value).

增益率准则可对取值数目较少的属性有所偏好.

\subsection{基尼指数}
% ### 4.2.3 基尼指数

CART决策树使用"基尼指数"(Gini index)来选择划分属性,数据集$D$纯度度量方式为
\begin{equation}
\text{Gini}(D)=\sum_{k=1}^{|\mathcal Y|}\sum_{k'\ne k}p_kp_{k'}=1-\sum_{k=1}^{|\mathcal Y|}p_k^2
\end{equation}

属性$a$的基尼指数为
\begin{equation}
\text{Gini\_index}(D,a)=\sum_{v=1}^V\frac{|D^v|}{|D|}\text{Gini}(D^v)
\end{equation}

\section{剪枝处理}
% ## 4.3 剪枝处理

决策树剪枝的策略有"预剪枝"(prepruning)和"后剪枝"(postpruning).预剪枝是在决策树生成过程中对每个结点在划分前进行估计,若当前结点的划分不能带来泛化性能的提升,则停止划分并将当前结点标记为叶节点;后剪枝则是先从训练集生成一颗完整的决策树,然后自底向上地对非叶结点进行考察,若将该结点对应的子树替换为叶节点能提升泛化性能,则将该子树替换为叶结点.

通常将数据集划分为训练集和验证集.采用预剪枝方法时,根据训练集选取最优属性,如果选取的最优属性可以提升验证集的精度时,则进行划分,否则不进行划分.采用后剪枝方法时,自底向上选择非叶结点,依次替换为叶结点,如果精度提升,则进行替换,否则不进行替换.

\section{连续与缺失值}
% ## 4.4 连续与缺失值

\subsection{连续值处理}
% ### 4.4.1 连续值处理

在C4.5决策树算法中,对于属性取值为连续值,最简单的处理方式采用二分法(bi-partition)对连续属性进行处理.设连续属性$a$在样本集$D$上出现了$n$个不同的取值$\{a^1,a^2,\dots,a^n\}$,基于划分点$t$可将$D$分为子集$D_t^-$和$D_t^+$,则划分点集合为$T_a=\{\frac{a^i+a^{i+1}}{2}|1\le i\le n-1\}$,可得信息增益
\begin{equation}\begin{split}
\text{Gain}(D,a)&=\max_{t\in T_a}\text{Gain}(D,a,t)\\
&=\max_{t\in T_a}\text{Ent}(D)-\sum_{\lambda\in\{-,+\}}\frac{|D_t^\lambda|}{|D|}\text{Ent}(D_t^\lambda)
\end{split}\end{equation}
其中$\max_{t\in T_a}\text{Gain}(D,a,t)$是样本集$D$基于划分点$t$二分后的信息增益.

与离散属性不同,若当前结点划分属性为连续属性,该属性还可作为其后代结点的划分属性.

\subsection{缺失值处理}
% ### 4.4.2 缺失值处理

缺失值处理主要解决以下两个问题:

\begin{enumerate}
\item 如何在属性值缺失的情况下进行划分属性选择;
\item 给定划分属性,若样本在该属性上的值缺失,如何对样本进行划分.
\end{enumerate}

对于问题1,将信息增益推广为
\begin{equation}
\text{Gain}(D,a)=\rho\times\text{Gain}(\tilde D,a)
\end{equation}
其中$\rho$为没有属性缺失值样本占所有样本的比例,$\tilde D$为无属性缺失值的样本集合.

对于问题2,需要将缺失属性值样本同时划分到不同的结点中,并调整P86定义的权重值,即让同一个样本以不同的概率概率划分到不同的子结点中去.

\section{多变量决策树}
% ## 4.5 多变量决策树

若把每个属性视为坐标空间中的一个坐标轴,则$d$个属性描述的样本就对应了$d$维空间中的一个数据点,决策树就是在这些数据点集合中寻找一个分类边界.决策树所形成的边界有一个特点即分类边界平行于坐标轴.

如果样本集合分布较为复杂,则需要较多的分类边界去拟合真实的分类边界.若能使用斜的分类边界,则决策树模型将大为简化."多变量决策树"(multivariate decision tree)就是能实现这样的"斜划分"甚至更复杂划分的决策树.