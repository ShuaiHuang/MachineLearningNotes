\chapter{强化学习}
% # 【《机器学习》第16章】强化学习(Reinforcement Learning)

\section{任务与奖赏}
% ## 16.1 任务与奖赏

在对机器做操作的过程中,仅仅能通过操作获得当前反馈,而难以判断最终奖赏的结果,对这个过程进行抽象,便得到了``强化学习''的模型.强化学习任务通常用马尔可夫决策过程(Markov Decision Process, MDP)来描述:机器处于环境$E$中,状态空间为$X$,每个状态$x\in X$为机器对当前环境的描述;机器所采取的动作空间为$A$,每个动作$a\in A$作用在当前状态$x$上,使得状态以概率$P$转移到另一个状态,环境根据``奖赏''(reward)函数$R$反馈给机器一个奖赏.综上,强化学习对应四元组$E=<X,A,P,R>$.需要注意的是,在环境中状态的转移,奖赏的返回是不受机器控制的,机器只能通过选择要执行的动作来影响环境,也只能通过观察转移后的状态和返回的奖赏来感知环境.

机器要做的事情就是在环境中不断尝试而学得一个``策略''(policy)$\pi$,根据这个策略,在状态$x$下就能得知要执行的动作$a=\pi(x)$.一种策略表示是函数$\pi:X\mapsto A$;另一种策略表示是$\pi:X\times A\mapsto \mathbbm R$.策略的优劣取决于长期执行这一策略后得到的累积奖赏.在强化学习任务中,学习的目的就是要找到能够使长期累积奖赏最大化的策略.

强化学习与有监督学习不同之处在于,在强化学习中并没有监督学习中的有标记样本,只有在学习过程最后得到最终结果.因此,强化学习在一定程度上可以看做``延迟标记信息''的监督学习问题.

\section{$K$-摇臂赌博机}
% ## 16.2 $K$-摇臂赌博机

\subsection{探索与利用}
% ### 16.2.1 探索与利用

欲最大化单步奖赏需要考虑两个方面:一是需要知道每个动作带来的奖赏,二是要执行奖赏最大的动作.

对于单步强化学习任务对应的``$K$-摇臂赌博机''(K-armed bandit),有两种策略:``仅探索''(expliration-only)法,将所有的尝试机会平均分给每个摇臂,最后以每个摇臂各自的平均吐币概率作为其奖赏期望的近似估计.``仅利用''(exploitation-only)法,按下目前最优的摇臂,若有多个摇臂同为最优,则从中随机选取一个.

``探索''和``利用''这两者是互相矛盾的,这就是强化学习所面临的``探索-利用窘境''(Exploration-Exploitation dilemma).欲使得累计奖赏最大,必须在探索与利用之间达成较好的折中.

\subsection{$\epsilon$-贪心}
% ### 16.2.2 $\epsilon$-贪心

$\epsilon$-贪心基于一个概率来对探索和利用进行折中:每次尝试时,以$\epsilon$的概率进行探索,以$1-\epsilon$的概率进行利用.

\subsection{Softmax}
% ### 16.2.3 Softmax

Softmax算法基于当前已知的摇臂平均奖赏来对探索和利用进行折中.若各摇臂的平均奖赏相当,则选取各摇臂的概率也相当;若某些摇臂的奖赏明显高于其他摇臂,则它们被选取的概率也明显要高.概率分配是基于Boltzmann分布:
$$P(k)=\frac{e^\frac{Q(k)}{\tau}}{\sum_{i=1}^Ke^\frac{Q(i)}{\tau}}$$
其中,$Q(i)$记录当前摇臂的平均奖赏;$\tau>0$称为``温度'',$\tau$越小则平均奖赏高的摇臂被选取的概率越高.

上述的$\epsilon$-贪心和Softmax算法都是针对单步强化学习问题的解决方案.如果简单地将多步强化学习看做多个单步强化学习的叠加,则会有很多局限性.因为它没有考虑强化学习任务中马尔可夫决策过程的结构.

\section{有模型学习}
% ## 16.3 有模型学习

如果任务对应的马尔科夫决策过程四元组$E=<X,A,P,R>$均已知，则称为``模型已知'',模型已知环境中的学习称为``有模型学习''（model-based learning）。

\subsection{策略评估}
% ### 16.3.1 策略评估

状态值函数（state value function）$V(\cdot)$评估的是从状态值$x$出发，使用策略$\pi$所带来的累积奖赏。

状态-动作值函数(state-action value function)$Q(\cdot)$评估的是从状态值$x$出发，执行动作$a$后再使用策略$\pi$所带来的累积奖赏。

由于MDP具有马尔可夫性质，即系统下一时刻的状态仅由当前时刻的状态决定，于是值函数有很简单的递归形式。

\subsection{策略改进}
% ### 16.3.2 策略改进

对策略进行评估的最终目的是为了判定其是否是最优策略，如果不是最优策略则需要对策略进行改进。理想的策略应该能最大化累积奖赏。
\begin{equation}
\pi^\ast={\arg\max}_\pi\sum_{x\in X}V^\pi(x)
\end{equation}

一个强化学习任务可能有多个最优策略，最优策略对应的值函数$V^\ast$称为最优值函数，即
\begin{equation}
\forall x\in X：V^\ast(x)=V^{\pi^\ast}(x)
\end{equation}
当策略空间无约束时$V^\ast$才是最优策略对应的值函数。如果策略空间有约束，则违背约束的策略是不合法的。

如果当前动作不是最优策略，则改进方式为：将策略选择的动作改变为当前最优的动作。

\subsection{策略迭代与值迭代}
% ### 16.3.3 策略迭代与值迭代

求解最优解的方法：从一个初始策略出发，先进行策略评估，然后改进策略，评估改进的策略，再进一步改进策略……不断迭代改进和评估，直到算法收敛为止。

策略迭代在每次改进策略后都需要重新进行策略评估，这比较耗费时间。

\section{免模型学习}
% ## 16.4 免模型学习

在现实的学习任务中，环境的转移概率、奖赏函数往往很难得知，如果算法不依赖于环境建模，则称为``免模型学习''（model-free learning)。

\subsection{蒙特卡罗强化学习}
% ### 16.4.1 蒙特卡罗强化学习

在免模型学习情形下，最大的问题就是策略无法评估。此时，只能通过在环境中执行不同的动作，来观察转移的状态和得到的奖赏。一种直接的策略评估替代方法是多次``采样''，然后求取平均累积奖赏来作为期望累积奖赏的近似，这称为蒙特卡罗强化学习。

策略迭代算法估计的是状态值函数$V$,而最终的策略是通过状态-动作值函数$Q$来获得。于是，需要将估计对象从$V$转变为$Q$，即估计每一对``状态-动作''的值函数。

由于在免模型学习情况下，只能从一个起始状态开始探索环境，因此只能在探索的过程中逐渐发现各个状态并估计各状态-动作对的值函数。

如果较好地获得值函数的估计，就需要多条不同的采样轨迹。获取不同采样轨迹的方法为（以$\epsilon-$贪心算法为例）：以$\epsilon$的概率从所有动作中均匀随机选取一个，以$1-\epsilon$概率选取当前最优动作。

如果被评估的策略与被改进的策略是同一个策略，则称之为``同策略''（on-policy）蒙特卡罗强化学习算法。然而，引入$\epsilon-$贪心算法是为了便于策略评估，实际上我们需要改进的策略是原始策略。为达到这一目的，可以借鉴以下思想：

一般的，函数$f$在概率分布$p$下的期望
\begin{equation}
\mathbbm E[f]=\int_xp(x)f(x)dx
\end{equation}
可通过从概率分布$p$上的采样$\{x_1,x_2,\dots,x_m\}$来估计$f$的期望，即
\begin{equation}
\mathbbm {\hat E}[f]=\frac{1}{m}\sum_{i=1}^mf(x_i)
\end{equation}
若引入另一个分布$q$，则函数$f$在概率分布$p$下的期望也可等价地写为
\begin{equation}
\mathbbm E=\int_x q(x)\frac{p(x)}{q(x)}f(x)dx
\end{equation}
上式可以通过在$q$上的采样$\{x_1',x_2',\dots,x_m'\}$估计为
\begin{equation}
\mathbbm {\hat E}[f]=\frac{1}{m}\sum_{i=1}^m\frac{p(x_i')}{q(x_i')}f(x_i')
\end{equation}
.

同样的，将上述分布$p,q$替换为对应的策略，就可以得到``异策略''（off-policy）蒙特卡罗强化学习算法。（见P386）

\subsection{时序差分学习}
% ### 16.4.2 时序差分学习

蒙特卡罗强化学习算法通过考虑采样轨迹，克服了模型未知给策略估计造成的困难。但是蒙特卡洛学习算法没有充分利用强化学习任务的MDP结构。时序差分（Temporal Difference, TD）学习则结合了动态规划与蒙特卡罗方法的思想，能做到更搞笑的学习。其本质就是在于将状态动作对的更新变成增量式的方式进行。（见P387）

典型代表算法有Sarsa算法，$Q$-学习算法。

\section{值函数近似}
% ## 16.5 值函数近似

前面所述的算法都是针对有限状态空间，现实的强化学习任务所面临的状态空间往往是连续的，有无穷多个状态。一个直接的想法就是对状态空间进行离散化，将连续状态空间转化为有限离散状态空间，但是这仍然是一个很难的问题。

事实上，不妨对连续状态空间的值函数进行学习。先**假定状态空间为$n$维实数空间$X=\mathbbm R^n$,并且值函数能表达为状态的线性函数**
\begin{equation}
V_{\mathbf \theta}(\mathbf x)=\mathbf\theta^T\mathbf x
\end{equation}
其中$\mathbf x$为状态向量，$\theta$为参数向量。由于此时的值函数难以像有限状态那样精确记录每个状态的值，因此这样值函数的求解被称为值函数近似（value function approximation）。

\section{模仿学习}
% ## 16.6 模仿学习

强化学习的经典任务设置中，机器所能获得的反馈信息仅有多步决策后的累积奖赏。如果在决策过程中参考决策范例进行决策的方式称为``模仿学习''（imitation learning）。

\subsection{直接模仿学习}
% ### 16.6.1 直接模仿学习

直接模仿学习通过将状态动作轨迹上的状态动作对抽取出来，构造出一个新的数据集合，然后使用分类或者回归算法学习得到新的模型，这种方式称为直接模仿学习。

\subsection{逆模仿学习}
% ### 16.6.2 逆模仿学习

通常设计奖赏函数非常困难，从人类专家提供的范例数据中反推出奖赏函数有助于解决该问题，这就是``逆强化学习''（inverse reinforcement learning）。

逆强化学习的基本思想是：欲使机器做出与范例一致的行为，等价于在某个奖赏函数的环境中求解最优策略，该最优策略所产生的轨迹与范例数据一致。

\section*{总结}

本章第一节首先介绍了\textit{强化学习}系统的主要构成:环境$E$;状态集合$X$,其中的每一个状态$x\in X$都是对环境$E$的抽象;动作集合$A$,其中$a\in A$是在所采取的动作;奖励$R$是采取某一个动作后所获得的奖赏;转移函数$P$描述了从当前状态转移到另一个状态的概率分布.从整体上来看,整个强化学习系统由四元组$E=<X,A,P,R>$组成.从学习过程上来看,由于在初始时刻很难确定样本所对应的标签,只能根据当前的环境和状态确定动作获取环境的反馈,经过若干轮反馈后才能获取到最终的标签.所以强化学习又被看做有延迟标记的有监督学习.学习的结果是输出策略$\pi$,根据这个策略就能得到下一步动作$a=\pi(x)$.评价策略好坏的指标是累计奖赏,能使长期累积奖赏最大化的策略就是我们的目标策略.

本章第二节将强化学习限定到单步强化学习的情形,并借助于$K$-摇臂赌博机这一理论模型引出了探索-利用窘境.为了能在探索和利用之间取得一个较好的折中,常用$\epsilon$-贪心法和Softmax算法.$\epsilon$-贪心算法以一定的概率$\epsilon$进行探索,以概率$1-\epsilon$进行利用;Softmax算法基于当前已知的摇臂平均奖赏对探索利用进行折中.两种算法的优劣主要取决于具体应用.如果要拓展到多步强化学习的情形,可以基于单步强化学习的情形进行简单叠加,将每一步的动作所对应的奖赏进行叠加得到长期累计奖赏.但是这样做并没有考虑到强化学习中马尔科夫决策过程的结果,具有很大的局限性.

第三节主要介绍了多步强化学习的情形,并假设四元组$E=<X,A,P,R>$均已知即\textit{模型已知}情形.这种学习被称为\textit{有模型学习}.对于学习到的策略$\pi$,需要有一个评价标准来判定学习结果的好坏.通常使用状态值函数$V(\cdot)$评价对当前状态使用策略所得到的累计奖赏;使用状态-动作函数$Q(\cdot)$评价对当前状态$x$使用动作$a$之后得到的累计奖赏.由于模型已知,因此可以对状态函数和状态-动作函数执行全概率展开\footnote{TODO:此处待展开描述}.在有了策略评价标注以后,就可以使用优化算法对策略进行改进.\footnote{TODO:Bella等式?最优Bella等式?}有了优化算法以后,就可以对策略进行迭代优化.

第四节将上一节中的模型已知情形进行拓展到\textit{免模型学习}情形.首先遇到的问题就是策略无法评估,进而导致无法做全概率展开.另一个问题是策略迭代算法估计的是状态值函数$V$,而最终的策略是通过状态-动作函数$Q$来获得.因此在这种情况下,就直接对状态-动作函数$Q$进行估计.此外,还需要在采样过程中采样出不同的状态轨迹,以学习到不同状态-动作对.如果被评估与被改进的是同一个策略,则被称为通策略蒙特卡罗强化学习算法;借助于重要性采样原理,还可以使得被评估和被采样策略不同,这种算法被称为异策略蒙特卡罗学习算法.蒙特卡罗学习算法是在采样得到整个采样轨迹后进行学习,时序差分学习算法则是使用增量式方法进行更新.

第五节将有限状态空间中的强化学习拓展到无线状态空间中,并提出了函数值金丝方法.

第六节则考虑借助于人类的专家知识进行模仿学习.一种方法是直接模仿学习,即借助于人类专家的决策轨迹数据进行训练.另一种方法是从人类专家的决策轨迹数据中反推出奖赏函数.