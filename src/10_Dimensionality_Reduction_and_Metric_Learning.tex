\chapter{降维与度量学习}
% # 【《机器学习》第10章】降维与度量学习

\section{$k$近邻学习}
% ## 10.1 $k$近邻学习

本章首先介绍了$k$近邻学习,但是并未介绍$k$近邻学习与降维或度量学习之间的关系.从逻辑上来说,$k$近邻学习只是为了从其缺点入手引入降维的必要性.

工作机制:给定测试样本,基于某种距离度量找出训练集中与其最近的$k$个样本,然后基于这$k$个``邻居''的信息来进行预测.$k$近邻学习是``懒惰学习''(lazy learning)的代表,训练过程仅仅是将训练样本进行存储,在收到测试样本后再进行处理;与之相对应的是``急切学习'',其在训练阶段就对样本进行处理.

给定测试样本$\mathbf{x}$,若其最近邻样本为$\mathbf{z}$,则最近邻分类器出错的概率就是$\mathbf{x}$与$\mathbf{z}$类别标记不同的概率.

\begin{equation}\begin{split}
P(err)&=1-\sum_{C\in\mathcal{Y}}P(c|\mathbf{x})P(c|\mathbf{z})\\&\simeq 1-\sum_{C\in\mathcal{Y}}P^2(c|\mathbf{x})\\&\le 1-P^2(c^\ast|\mathbf{x})\\&=(1+P(c^\ast|\mathbf{x}))(1-P(c^\ast|\mathbf{x}))\\&\le 2\cdot(1-P(c^\ast|\mathbf{x}))
\end{split}\end{equation}
其中,$c^\ast={\arg\max}_{c\in\mathcal{Y}}P(c|\mathbf{x})$表示贝叶斯最优分类器分类结果.

\section{低维嵌入}
% ## 10.2 低维嵌入

本节首先明确了上一节中的一个重要前提,即$k$近邻成立的前提是训练样本集是经过``密采样''(dense sample)得到的.但是在实际中``密采样''会产生计算方面的问题.因此降维的必要性就凸显出来.

上一节的重要假设:任意测试样本$\mathbf{x}$附近任意小的$\delta$距离范围内总能找到一个训练样本,即训练样本的采样密度足够大.

在高维情况下出现的数据样本稀疏,距离计算等问题,是所有机器学习方法面临的严重障碍,被称为``维数灾难''(curse of dimensionality).

若在降维的过程中要求原始样本空间中的距离在低维空间中得以保存,就得到了``多维缩放''(Multiple Dimensional Scaling)降维方法.

定义:  
$\mathbf{D}\in\mathbbm{R}^{m\times m}$:$m$个样本原始空间距离矩阵;  
$\mathbf{Z}\in\mathbbm{R}^{d'\times m}$:降维后中心化的样本;  
$\mathbf{B}=\mathbf{Z^TZ}\in\mathbbm{R}^{m\times m}$:降维后的内积矩阵.

样本$i$与样本$j$之间距离的平方(降维前后相等)
\begin{equation}\begin{split}
dist_{ij}^2&=\|\mathbf{z_i}\|^2+\|\mathbf{z_j}\|^2-2\mathbf{z_i^Tz_j}\\
&=b_{ii}+b_{jj}-2b_{ij}
\end{split}\end{equation}
经过代换得$b_{ij}=-\frac{1}{2}(dist_{ij}^2-dist_{i\cdot}^2-dist_{\cdot j}^2+dist_{\cdot\cdot}^2)$,由此求得$\mathbf{B}$,对$\mathbf{B}$做特征值分解$\mathbf{B=V\Lambda V^T}$,选取出其中的$d^\ast$个非零特征值有
\begin{equation}
\mathbf{Z=\Lambda_{\ast}^{\frac{1}{2}}V_{\ast}^T}\in\mathbbm{R}^{d^\ast\times m}
\end{equation}
其中,$\Lambda=diag(\lambda_1, \lambda_2, \dots, \lambda_d)$是特征对角矩阵;$\mathbf{V}$是特征向量矩阵.

一般来说,欲获得低维子空间,最简单的是对原始高维空间进行线性变换,又称线性降维.它们都符合$\mathbf{Z=W^TX}$的形式,不同之处是对低维子空间的性质有不同要求,相当于对变换矩阵$\mathbf{W}$加上了不同约束.

评估方式通常是比较降维前后学习器性能,性能提升则认为降维起了作用.

\section{主成分分析}
% ## 10.3 主成分分析

主成分分析(Principal Component Analysis)是最常用的一种降维方法.

正交空间降维到超平面应该满足:
\begin{itemize}
\item 最近重构性:样本点到这个超平面的距离足够近;
\item 最大可分性:样本点在这个超平面上的投影能尽可能分开.
\end{itemize}

定义:  
投影变换后,新坐标系为$\{\mathbf{w_1, w_2, \dots, w_d}\}$,其中$\mathbf{w_i}$是标准正交基向量;  
降维的目的是丢弃部分基向量,降低维度至$d'< d$;  
$\mathbf{z_i}$是$\mathbf{x_i}$在低维空间中投影.

根据最近重构性,有
\begin{equation}\begin{split}
\sum_{i=1}^m\|\sum_{j=1}^{d'}z_{ij}\mathbf{w}_j-\mathbf{x}_i\|_2^2&=\sum_{i=1}^m\mathbf{z}_i^T\mathbf{z}_i-2\sum_{i=1}^m\mathbf{z}_i^T\mathbf{W}^T\mathbf{x}_i+const\\&\propto -tr\Big(\mathbf{W}^T(\sum_{i=1}^m\mathbf{x}_i\mathbf{x}_i^T)\mathbf{W}\Big)
\end{split}\end{equation}
将上式最小化,并考虑到$\mathbf{w}_j$是标准正交基,$\sum_{i=1}^m\mathbf{x}_i\mathbf{x}_i^T$是协方差矩阵,有
\begin{equation}\begin{split}
\min_\mathbf{w}&-tr\Big(\mathbf{W^TXX^TW}\Big)\\
s.t.& \mathbf{W^TW=I}
\end{split}\end{equation}

根据最大可分性,$\mathbf{x}_i$在超平面上投影$\mathbf{W}^T\mathbf{x}_i$,投影后样本点方差为$\sum_i\mathbf{W}^T\mathbf{x}_i\mathbf{x}_i^T\mathbf{W}$,于是有
\begin{equation}\begin{split}
\max_{\mathbf{W}}&\,tr(\mathbf{W}^T\mathbf{XX}^T\mathbf{W})\\
s.t.&\,\mathbf{W}^T\mathbf{W=I}
\end{split}\end{equation}

由此可见,最近重构性与最大可分性二者等价.

PCA舍弃$d-d'$个特征值是降维结果.一方面,采样密度增加;另一方面,最小特征值往往与噪声有关,舍弃他们能在一定程度上起到去噪的效果.

\section{核化线性降维}
% ## 10.4 核化线性降维

前一节中提到的降维方式是线性降维,即假设从高维空间到低维空间的函数映射是线性的,然而这与实际情况的差别很大,现实中有不少情况需要非线性降维,这样可以保持原采样空间中的低维结构.

注:
\begin{itemize}
\item 对样本空间进行采样会造成维度增加;
\item 原本采样的低维空间称为``本真''(intrinsic)低维空间.
\end{itemize}

非线性降维的一种常用方法,是基于核技巧对线性降维方法进行``核化''(kernelized).

通常采用的PCA方法对数据降维的原理是
\begin{equation}
\Big(\sum_{i=1}^m\mathbf{z_iz_i}^T\Big)\mathbf{W}=\lambda\mathbf{W}
\end{equation}
其中$\mathbf{W}$是数据降维后投影的超平面;$\mathbf{z_i}$是样本点$\mathbf{x}_i$在高维特征空间中通过映射$\phi$产生的.因此有
\begin{equation}
\Big(\sum_{i=1}^m\phi(\mathbf{x}_i)\phi(\mathbf{x}_i)^T\Big)\mathbf{W}=\lambda\mathbf{W}
\end{equation}
但是由于$\mathbf{x}_i$在一般情况下是未知的,所以$\phi$的具体形式也并不清晰.由此,引入核函数$\kappa(\mathbf{x}_i, \mathbf{x}_j)=\phi(\mathbf{x}_i)^T\phi(\mathbf{x}_j)$,代入上式有
\begin{equation}
\mathbf{KA}=\lambda\mathbf{A}
\end{equation}
其中,$\mathbf{K}$为$\kappa$对应的核矩阵;$\mathbf{A}=(\alpha_1, \alpha_2, \dots, \alpha_m)$.

为获得投影后的坐标KPCA需对所有样本求和,因此它的计算开销较大.

\section{流形学习}
% ## 10.5 流形学习

流形是在局部与欧式空间同胚的空间,它在局部具有欧式空间的性质,能用欧氏距离来进行计算.

\subsection{等度量映射(Isometric Mapping)}
% ### 10.5.1 等度量映射(Isometric Mapping)

等度量映射的基本出发点,是认为低维流形嵌入到高维空间后,直接在高维空间中计算直线距离具有误导性.因为高维空间中的直线距离在低维嵌入的流形上是不可达的.通常在流形上使用``测地线''(geodesic)距离.这样距离计算就转化为计算邻阶图上两点间的最短距离.可以使用经典的Djikstra算法或者Floyd算法.

新样本映射到低维空间中的方法:降训练样本中的高维空间坐标作为输入,低维空间中的坐标作为输出,训练一个回归学习器来对新样本的低维空间坐标进行预测.

近邻图构造方法有两种:指定近邻点个数进行构造;指定距离阈值进行构造.

\subsection{局部线性嵌入(Locally Linear Embedding)}
% ### 10.5.2 局部线性嵌入(Locally Linear Embedding)

LLE试图保持邻域样本间的线性关系.其步骤为:
\begin{enumerate}
\item 求取重构系数$\mathbf{w}_i$
\begin{equation}\begin{split}
\min_{\mathbf{w}}&\sum_{i=1}^m\|\mathbf{x}_i-\sum_{j\in Q}w_{ij}\mathbf{x}_j\|^2_2\\
s.t.& \sum_{j\in Q}w_{ij}=1
\end{split}\end{equation}
\item 求取低维坐标$\mathbf{z}_i$
\begin{equation}
\min_{\mathbf{z}_1, \mathbf{z}_2, \dots, \mathbf{z}_m}\sum_{i=1}^m\|\mathbf{z}_i-\sum_{j\in Q}w_{ij}\mathbf{z}_j\|_2^2
\end{equation}
\end{enumerate}

\section{度量学习}
% ## 10.6 度量学习

高维数据降维的主要目的是通过找到一个合适的低维空间,在此空间中的学习器性能比原始空间中学习器的性能要好.实质上,就是寻找合适的空间,寻找一个合适的距离度量.

马氏距离(Mahalanobis Distance)的定义为:
\begin{equation}\begin{split}
dist_{mah}(\mathbf{x}_i, \mathbf{x}_j)&=(\mathbf{x}_i -\mathbf{x}_j)^T\mathbf{M}(\mathbf{x}_i -\mathbf{x}_j)\\&=\|\mathbf{x}_i -\mathbf{x}_j\|_{\mathbf{M}}^2
\end{split}\end{equation}
其中$\mathbf{M}$是度量矩阵,是度量学习的目标.为保持距离非负性和对称性,$\mathbf{M}$必须是(半)正定.

除了把错误率作为度量学习的目标,还能在度量学习中引入领域知识,如定义``必连''(must-link)约束集合与``勿连''(cannot-link)约束集合.

若学习得到的$\mathbf{M}$是一个低秩矩阵,通过对$\mathbf{M}$进行特征值分解,找到一组正交基,数目为$rank(\mathbf{M})$,小于属性数目$d$.于是度量学习的结果可以衍生出一个降维矩阵$P\in\mathbbm{R}^{d\times Rank(\mathbf{M})}$用于降维.