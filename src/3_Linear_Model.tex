\chapter{线性模型}

\section{基本形式}
% ## 3.1 基本形式

线性模型(Linear Model)本质上就是一个试图学习出属性线性组合来进行预测的函数.一般向量形式为
\begin{equation}
    f(\bm x)=\bm{w}^T\bm{x}+b
\end{equation}
学得$\bm w$和$b$后,模型就能被确定.

\section{线性回归}
% ## 3.2 线性回归

``线性回归''(linear regression)试图学得一个线性模型以尽可能准确地预测实值输出标记.一般使用最小二乘法找到一条直线,使得各个样本到直线上的距离之和最小.用矩阵形式表示为
\begin{equation}
\hat{\bm w}=\mathop{\arg\min}_{\hat{\bm w}}(\bm y-\bm X\hat{\bm w})^T(\bm y-\bm X\hat{\bm w})
\end{equation}
当$\bm X^T\bm X$为满秩矩阵或正定矩阵时,有闭式解
\begin{equation}
\hat{\bm w}^\ast=(\bm X^T\bm X)^{-1}\bm X^T\bm y
\end{equation}
当$\bm X^T\bm X$不是满秩矩阵或正定矩阵时,有多个解,选择哪个解作为输出由算法的归纳偏好确定,常见的做法是引入正则化(regularization)项.

同样也可以让线性模型逼近$y$的衍生物,如
\begin{equation}
\ln y=\bm w^T\bm x+b
\end{equation}
这就是对数线性回归(log-linear regression).更一般的,考虑单调可微函数$g(\cdot)$,令
\begin{equation}
y=g^{-1}(\bm w^T\bm x+b)
\end{equation}
这样得到的模型称为``广义线性模型''(generalized linear model).

\section{对数几率回归}
% ## 3.3 对数几率回归

对数几率回归又称\textit{Logistic Regression},虽然名字中带着回归两个字，但它是一个分类模型。如果要做的是分类任务,只需要找一个单调可微函数将分类任务的真实标记$y$与线性回归模型的预测值联系起来.最理想的是``单位阶跃函数''(unit-step function),但是由于其不是连续的,所以需要找到一个``替代函数''(surrogate function).对数几率函数(logistic function)正是这样一个函数.
\begin{equation}
y=\frac{1}{1+e^{-(\bm w^T\bm x+b)}}
\end{equation}
估计$\bm w,b$的过程见P59.

\section{线性判别分析}
% ## 3.4 线性判别分析

线性判别分析(Linear Discriminant Analysis, LDA)的思想为:给定训练样例集,设法将样例投影到一条直线上,使得同类样例的投影点尽可能接近,异类样例投影点尽可能远离.

令$X_i,\mu_i,\Sigma_i$分别表示第$i$类示例的集合,均值向量,协方差矩阵.若将数据投影到直线$\bm w$上,则两类样本的中心在直线上的投影分别为$\bm w^T\bm\mu_0$和$\bm w^T\bm\mu_1$;若将所有样本点都投影到直线上,则两类样本的协方差分别为$\bm w^T\bm\Sigma_0\bm w$和$\bm w^T\bm\Sigma_1\bm w$.则可得到最大化的目标
\begin{equation}\begin{split}
J&=\frac{\|\bm w^T\bm\mu_0-\bm w^T\bm\mu_1\|_2^2}{\bm w^T\bm\Sigma_0\bm w+\bm w^T\bm\Sigma_1\bm w}\\
&=\frac{\bm w^T(\bm\mu_0-\bm\mu_1)(\bm\mu_0-\bm\mu_1)^T\bm w}{\bm w^T(\bm\Sigma_0+\bm\Sigma_1)\bm w}
\end{split}\end{equation}
定义类内散度矩阵(within-class scatter matrix)
\begin{equation}
S_w=\Sigma_0+\Sigma_1
\end{equation}
以及类间散度矩阵(between-class scatter matrix)
\begin{equation}
S_b=\bm{(\mu_0-\mu_1)(\mu_0-\mu_1)}^T
\end{equation}
求解过程见P61-62.

\section{多分类学习}
% ## 3.5 多分类学习

在实际中,有些二分类学习方法可以直接推广到多分类,但是在大部分情形下,我们利用二分类学习器来解决多分类问题.通常采用的方法是``拆解法'',即将多分类任务拆解成二分类任务.常用的拆解策略有:
\begin{itemize}
\item 一对一(One vs. One, OvO):训练$N(N-1)/2$个分类器将$N$个类别两两分开;
\item 一对其余(One vs. Rest, OvR):训练$N$个分类器,将每个样本与其余的样本分开;
\item 多对多(Many vs. Many, MvM):常见的有``纠错输出编码''(Error Correcting Output Codes, ECOC)技术.见P65.
\end{itemize}

ECOC工作过程分为两步:编码和解码.在编码过程中,对$N$个类别做$M$次划分,每次划分将一部分类别划为正类,一部分划分为反类;解码过程中,使用$M$个分类器对测试样本进行预测,这些预测标记组成一个编码,然后将预测编码与每个类别各自的编码进行比较,返回距离最小的类别作为最终的预测结果.

ECOC在码长较小时可以根据这个原则计算出理论最优编码.然而码长较长时就难以有效地确定最优编码,这也是个NP难问题.

\section{类别不平衡问题}
% ## 3.6 类别不平衡问题

类别不平衡(class-imbalance)就是指分类任务中不同类别的训练样例数目差别很大的情况.从线性分类器角度来看,用$y=\bm{w^Tx}+b$对样本$\bm x$进行分类时,实际上是用$y$与某一个阈值进行比较.通常用$0.5$作为阈值.但是由于我们通常假设训练集是真实样本总体的无偏采样,因此有如下的``再平衡''策略:

观测几率就代表了真实几率,设$m^+$和$m^-$分别为正例数目和反例数目,判别形式为
\begin{equation}\label{eq:rescaling}
\text{若}\frac{y}{1-y}>\frac{m^+}{m^-}\text{时,预测为正例}
\end{equation}

由于我们未必能基于训练集观测几率来推断出真实的几率,现在有三种做法:
\begin{itemize}
\item 直接对训练集里的反类样例进行欠采样(undersampling)
\item 对训练集里的正类样例进行过采样(oversampling)
\item 直接基于原始训练集进行学习,将式(\ref{eq:rescaling})所代表的再平衡策略纳入到决策过程中,称为``阈值移动''(shreshold-moving).
\end{itemize}

\section*{总结}

机器学习有四要素:数据集上特征提取方式,损失函数,优化过程和模型.从本章开始,注意总结各种模型的损失函数和优化过程.

\begin{table}[!bht]
\caption{模型总结}
\centering
\begin{tabular}{|r|c|c|c|c|}
    \hline
    \textbf{模型} & \textbf{损失函数} & \textbf{优化过程} & \textbf{判别函数} \\ 
    \hline
    线性模型&均方误差MSE&最小二乘法，有闭式解&$f(\bm x)=\bm w^T\bm x+b$\\
    \hline
    Logistic回归&最大化对数似然&梯度下降法,牛顿法等&\\
    \hline
    线性判别分析&广义瑞利商&拉格朗日乘子法&\\
    \hline
\end{tabular}
\end{table}
