\chapter{特征选择与稀疏学习}
% # 【《机器学习》第11章】特征选择与稀疏学习

\section{子集搜索与评价}
% ## 11.1 子集搜索与评价

我们将属性称为``特征''(feature),对当前学习任务有用的属性称为``相关特征''(relevant feature),没什么用的属性称为``无关特征''(irrelevant feature).从给定的特征集合中选择出相关特征子集的过程叫做``特征选择''(feature selection).

特征选择是一个重要的``数据预处理''(data preprocessing)过程.进行特征选择的原因有:第一,避免维数灾难;第二,去除不相关的特征降低学习任务难度.需要取出的特征有两种:第一种是无关特征,第二种是冗余特征(需要视情况而定).

选取特征子集的问题是一个NP难问题,因此不能通过随机搜索方法来求得最优解.为了通过启发式搜索来获得次优解,需要解决两个问题:第一,如何根据评价结果选取下一个特征子集;第二,如何评价候选特征子集.

对于第一个``子集搜索''(subset search)问题,采用贪心算法进行特征子集的搜索,通常使用的策略有:``前向''(foward)搜索,``后向''(backward)搜索和``双向''(bidirection)搜索,但是这些搜索策略并未考虑特征之间的关联.

对于第二个``子集评价''(subset evalution)问题,只要评估特征子集划分的信息增益即可.设$\mathcal{D}$是样本集合;$p_i$是$\mathcal{D}$中第$i$类样本所占比例.对于属性子集$A$,根据取值将$\mathcal{D}$划分为$\{\mathcal{D}^1,\mathcal{D}^2,\dots,\mathcal{D}^V\}$,则属性子集增益为
\begin{equation}
Gain(A)=Ent(\mathcal{D})-\sum_{i=1}^V\frac{|\mathcal{D}^i|}{|\mathcal{D}|}Ent(\mathcal{D}^i)
\end{equation}
其中,$Ent(\mathcal{D})=-\sum_{i=1}^{|\mathcal{Y}|}p_k\log_2p_k$.

一般的,$A$确定了对$\mathcal{D}$的一个划分;$\mathcal{Y}$是样本标记,确定了对样本集$\mathcal{D}$的真实划分,只要能量化这两个划分的差异,就能对$A$进行评价.

子集搜索和子集评价结合起来就是特征选择.常见的特征选择策略有:过滤式(filter),包裹式(wrapper)和嵌入式(embedding).

\section{过滤式选择}
% ## 11.2 过滤式选择

过滤式特征选择先对特征集合进行特征选择,然后再训练学习器,特征选择过程与后续学习器训练过程无关.

Relief特征选择方法是过滤式的特征选择方法,该方法使用``相关统计量''来度量特征的重要性.假设有训练集$\{(\mathbf{x}_1, y_1),(\mathbf{x}_2, y_2),\dots,(\mathbf{x}_m, y_m)\}$,对于每个实例$\mathbf{x}_i$,Relief先在$\mathbf{x}_i$的同类样本中寻找其最近邻$\mathbf{x}_{i,nh}$,称为``猜中近邻''(near-hit);再从异类中寻找其最近邻$\mathbf{x}_{i,nm}$,称为``猜错近邻''(near-miss),则相关统计量对应于属性$j$的分量为
\begin{equation}
\delta^j=\sum_i-\text{diff}(x_i^j,x_{i,nh}^j)^2+\text{diff}(x_i^j,x_{i,nm}^j)^2
\end{equation}
相关统计量分量值越大,对应属性的分类能力越强.

Relief-F是Relief的多类变体,能处理多类问题.
\begin{equation}
\delta^j=\sum_i-\text{diff}(x_i^j,x_{i,nh}^j)^2+\sum_{l\ne k}\Big(p_l\times\text{diff}(x_l^j,x_{l,nm}^j)^2\Big)
\end{equation}
其中,$p_l$为第$l$类样本在数据集$\mathcal{D}$中所占的比例.

\section{包裹式选择}
% ## 11.3 包裹式选择

包裹式特征选择直接把最终将要使用的学习器性能作为特征子集的评价准则.包裹式特征选择的目的就是为给定学习器选择最有利于其性能的特征子集.

从学习器性能来看,包裹式比过滤式要好;从计算开销来看,由于需要多次训练学习器,包裹式的开销比过滤式要大.

LVW(Las Vegas Wrapper)是在拉斯维加斯框架下使用随机策略进行搜索.若有时间限制,则不一定能给出满足要求的解.与之相对应的,蒙特卡罗方法一定会给出解.

\section{嵌入式选择与L1正则化}
% ## 11.4 嵌入式选择与L1正则化

嵌入式特征选择是将特征选择过程与学习器训练过程融为一体,即在学习器训练过程中自动地进行特征选择.

对于线性回归模型,岭回归(Ridge Regression)的优化目标为
\begin{equation}
\min_{w}\sum_{i=1}^m(y_i-\mathbf{w}^T\mathbf{x}_i)^2+\lambda\|\mathbf{w}\|_2^2
\end{equation}
如果将正则化项中$2-$范数替换为$p-$范数,特别的,当$p=1$时,有LASSO(Least Absolute Shrinkage and Selection Operator)
\begin{equation}
\min_{\mathbf{w}}\sum_{i=1}^m(y_i-\mathbf{w^Tx_i})^2+\lambda\|\mathbf{w}\|_1
\end{equation}
相比于岭回归,LASSO更容易获得``稀疏''(sparse)解.这意味着$\mathbf{w}$的非零分量对应的特征最终才在模型中得以体现,即进行了特征选择.

求解$L1$正则化问题可使用近端梯度下降(Proximal Gradient Descent, PGD)方法.对优化目标
\begin{equation}
\min_\mathbf{x}f(\mathbf{x})+\lambda\|\mathbf{x}\|_1
\end{equation}
若$f(\mathbf{x})$可导且$\nabla f(\mathbf{x})$满足L-Lipschitz条件则可求解出次优解.其中,L-Lipschitz条件为存在$L>0$,使得
\begin{equation}
\|\nabla f(\mathbf{x}')-\nabla f(\mathbf{x})\|_2^2\le L\|\mathbf{x}'-\mathbf{x}\|_2^2,\forall\mathbf{x},\mathbf{x}'
\end{equation}

求解过程迭代公式为
\begin{equation}
\mathbf{x}_{k+1}={\arg\min}_{\mathbf{x}}\frac{L}{2}\|\mathbf{x-z}\|_2^2+\lambda\|\mathbf{x}\|_1
\end{equation}
其中$\mathbf{z=x}_k-\frac{1}{L}\nabla f(\mathbf{x}_k)$

易知$\mathbf{x}$各个分量互不影响,则有闭式解
\begin{equation}
x_{k+1}^i=
\begin{cases}
z^i-\lambda/L,&\lambda/L<z^i\\
0,&|z^i|\le \lambda/L\\
z^i+\lambda/L,& z^i<-\lambda/L
\end{cases}
\end{equation}

推导闭式解的过程:
\begin{equation}
\begin{split}
x_{k+1}^i&={\arg\min}_{x^i}\frac{L}{2}(x^{i2}-2x^iz^i+z^{i2})+\lambda|x^i|\\
&=\begin{cases}
\frac{L}{2}(x^{i2}-2x^iz^i+z^{i2})+\lambda|x^i|,&x^i\ge0\\
\frac{L}{2}(x^{i2}-2x^iz^i+z^{i2})-\lambda|x^i|,&x^i<0
\end{cases}\end{split}
\end{equation}
对于上述二次曲线(抛物线),取得最小值点,并与条件$x^i\ge0$或$x^i<0$进行综合判断.

\section{稀疏表示与字典学习}
% ## 11.5 稀疏表示与字典学习

特征选择所考虑的问题是特征具有稀疏性,即从特征集中选择出与学习任务相关的特征子集.但是从另外一个角度来看,如果训练样本对应的特征向量具有稀疏性,那么在高维空间中样本分布就有可能线性可分,学习器可能会取得较好的性能.此外,由于样本是稀疏的,计算和存储的开销会比较小.

由于稀疏特征具有以上的优点,因此,如何学习出一个``字典'',为稠密表达的样本转化为稀疏形式,是``字典学习''(directoinary learning),又称``稀疏编码''(sparse coding)所要解决的问题.

给定数据集$\{\mathbf{x}_1,\mathbf{x}_2,\dots,\mathbf{x}_m\}$,稀疏编码的模型为
\begin{equation}
\min_{\mathbf{B,\alpha}_i}\sum_{i=1}^m\|\mathbf{x}_i-\mathbf{B\alpha}_i\|_2^2+\lambda\|\alpha_i\|_1
\end{equation}
上述问题可通过交替变量优化法求解:
\begin{enumerate}
\item 固定$\mathbf{B}$,为每个样本$\mathbf{x}_i$求解对应$\alpha_i$
    \begin{equation}
    \min_{\alpha_i}\|\mathbf{x}_i-\mathbf{B}\alpha_i\|_2^2+\lambda\|\alpha_i\|_1
    \end{equation}
\item 以$\alpha_i$为初值更新$\mathbf{B}$
    \begin{equation}
    \min_{\mathbf{B}}\|\mathbf{X-BA}\|_F^2
    \end{equation}
\end{enumerate}
其中$\|\cdot\|_F$是矩阵的Frobenius范数,上述优化问题常用于基于逐列更新策略的KSVD方法求解.

\section{压缩感知}
% ## 11.6 压缩感知

压缩感知所要解决的问题是从部分信息中恢复出全部的信号.

原问题可描述为:假定有长度为$m$的离散信号$\mathbf{x}$,不放假定以远小于奈奎斯特采样定理要求的采样率进行采样,得到长度为$n$的采样后信号$\mathbf{y}$,$n\ll m$,即
\begin{equation}
\mathbf{y=\Phi x}
\end{equation}
其中$\Phi\in \mathbb{R}^{n\times m}$是对信号$\mathbf{x}$的测量矩阵.

不妨假定存在某个线性变换$\Psi\in\mathbb{R}^{m\times m}$,使得$\mathbf{x}$可表示为
\begin{equation}
\mathbf{y=\Phi\Psi s=As}
\end{equation}
其中$\mathbf{A=\Phi\Psi}\in\mathbb{R}^{n\times m}$.于是,若能根据$\mathbf{y}$恢复出$\mathbf{s}$,则可通过$\mathbf{x=\Psi s}$来恢复出信号$\mathbf{x}$.

更进一步,若$\mathbf{s}$具有稀疏性,这个问题就很好解.实际上,获得稀疏解$\mathbf{s}$的途径有很多.

通常认为,压缩感知分为``感知测量''和``重构恢复''两个阶段.前者是将原始信号进行处理以获得稀疏表示的过程;后者是从稀疏表示中恢复出原始信号的过程.

压缩感知中一个重要的原理是``限定等距性''(Restricted Isometry Property, RIP):对于$n\times m(n\ll m)$的矩阵$\mathbf{A}$,若存在常数$\delta_k\in(0,1)$,使得对于任意向量$\mathbf{s}$和$\mathbf{A}_k\in\mathbb{R}^{n\times k}$有
\begin{equation}
(1-\delta_k)\|\mathbf{s}\|_2^2\le \|\mathbf{A}_k\mathbf{s}\|_2^2\le (1+\delta_k)\|\mathbf{s}\|_2^2
\end{equation}
则称$\mathbf{A}$满足$k-$RIP.进而可从下列问题中求解出$\mathbf{s}$,并恢复出$\mathbf{x}$
\begin{equation}
\begin{split}\min_{\mathbf{s}}&\|\mathbf{s}\|_0\\
s.t.& \mathbf{y=As}\end{split}
\end{equation}

上述问题涉及到$0$范数,是NP难问题,但该问题与
\begin{equation}
\begin{split}\min_{\mathbf{s}}&\|\mathbf{s}\|_1\\
s.t.& \mathbf{y=As}\end{split}
\end{equation}
问题共解.

另外,本节中所举的例子还涉及到矩阵补全(matrix completion)技术,即通过解
\begin{equation}
\begin{split}
\min_{\mathbf{X}}&rank(\mathbf{X})\\
s.t.&(\mathbf{X})_{ij}=(\mathbf{A})_{ij},(i,j)\in\Omega
\end{split}
\end{equation}
其中$\Omega$是已观测值下标集合;$\mathbf{X}$是需要恢复的信号;$\mathbf{A}$是包含缺失值的观测信号.该问题也是NP难问题,但与下列问题同解
\begin{equation}
\begin{split}
\min_{\mathbf{X}}&\|\mathbf{X}\|_{\ast}\\
s.t.&(\mathbf{X})_{ij}=(\mathbf{A})_{ij},(i,j)\in\Omega
\end{split}
\end{equation}
其中$\|\mathbf{X}\|_{\ast}=\sum_{j=1}^{\min\{m,n\}}\sigma_j(\mathbf{X})$.其中,$\sigma_j(\mathbf{X})$是$\mathbf{X}$的奇异值.可通过半正定规划(Semi-Definite Programming, SPD)求解.