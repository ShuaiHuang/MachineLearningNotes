\chapter{神经网络}
% # 【《机器学习》第5章】神经网络

\section{神经元模型}
% ## 5.1 神经元模型

神经网络的基本单元为如P97底部图5.1所示的M-P神经元模型,其数学模型可以抽象为
\begin{equation}
y=f\Big(\sum_{i=1}^nw_ix_i-\theta\Big)
\end{equation}
其中,$x_i$是当前神经元的输入信号;$w_i$是上一层第$i$个神经元与当前神经元的连接权重;$\theta$为当前神经元的激活阈值;$f(\cdot)$是激活函数.理论上最优的激活函数是阶跃函数,但是由于其数学性质并不是特别好,所以一般使用Sigmoid函数作为最优近似.

从数学角度上看,神经网络就是由大量的$f(\sum_iw_ix_i-\theta)$嵌套而得.

\section{感知机与多层网络}
% ## 5.2 感知机与多层网络

感知机(Perceptron)由两层神经元组成,输入层接收外界输入信号传递给输出层,输出层是M-P神经元.需要从训练数据集中训练得到输出层权重$w_i$以及阈值$\theta$.可以将阈值$\theta$看做一个输入权重固定为$-1$的``哑结点''(dummy node),这样权重和阈值的学习过程就可以归一化为权重的学习.对于训练样例$(\mathbf x,y)$,当前感知机的输出为$\hat y$,则感知机权重调整策略为\footnote{这里是感知机权重调整策略}
\begin{equation}\begin{split}
w_i&\leftarrow w_i+\bigtriangleup w_i \\
\bigtriangleup w_i&=\eta(y-\hat y)x_i
\end{split}\end{equation}
其中$\eta\in (0,1)$称为学习率.

感知机只有输出层神经元进行激活函数处理,学习能力非常有限.要解决非线性可分问题,就要使用多层功能神经元,通常使用的是``多层前馈神经网络''.

\section{误差逆传播算法}
% ## 5.3 误差逆传播算法

要训练多层神经网络,最常用的是学习算法是误差逆传播(error BackPropagation, BP)算法.误差逆传播算法对\textbf{均方误差}求取目标的负梯度方向对参数进行调整\footnote{广义感知器学习规则},调整方向为从输出层开始逐步调整到输入层\footnote{链式求导法则}.

标准BP算法每次针对一个训练样本进行参数优化,更新频率较快.为了解决这一问题,使用累积BP算法,即对所有样本求取累积均方误差,然后更新一次参数.

BP神经网络经常遭遇过拟合,有两种策略可以解决过拟合问题:

\begin{itemize}
\item ``早停''(early stopping):将数据分为训练集和验证集,若训练集误差降低但是验证集误差升高,则停止训练;
\item ``正则化''(regularization):将误差目标函数中增加一个用于描述网络复杂度的部分.
\end{itemize}

\section{全局最小与局部极小}
% ## 5.4 全局最小与局部极小

基于梯度的搜索是使用最为广泛的参数寻优方法.如果误差函数具有多个局部极小,则不能保证找到的解是全局最小.通常采用的``跳出''局部极小的方法有:

\begin{itemize}
\item 以多组不同参数值初始化多个神经网络,从中选取更有可能获得更接近全局极小值的结果;
\item 使用``模拟退火''(stimulated annealing)技术,在每一步以一定的概率接受比当前解更差的结果;
\item 使用随机梯度下降.
\end{itemize}

遗传算法(genetic algorithms)也常用来训练神经网络以更好地逼近全局最小.

但是上述方法大多是启发式,缺乏理论保障.

\section{其他常见的神经网络}
% ## 5.5 其他常见的神经网络

\subsection{RBF网络}
% ### 5.5.1 RBF网络

RBF(Radial Basis Function,径向基函数)是一种单隐层前馈神经网络,它使用径向基函数作为隐层神经元激活函数.具有足够多隐层神经元的RBF网络能以任意精度逼近任意连续函数.

\subsection{ART网络}
% ### 5.5.2 ART网络

竞争型学习(competitive learning)是神经网络中一种常用的无监督学习策略.网络的输出神经元相互竞争,每一时刻仅有一个竞争获胜的神经元被激活,其他神经元状态被抑制.

ART网络比较好地缓解了竞争型学习中的``可塑性-稳定性窘境''(stability-plasticity dilemma),这就使得ART网络具有一个很重要的优点:可进行增量学习(incremental learning)或在线学习(online learning).

\subsection{SOM网络}
% ### 5.5.3 SOM网络

SOM(Self-Organizing Map,自组织映射)网络是一种竞争学习型无监督神经网络,它能将高维输入数据映射到低维空间,同时保持输入数据在高维空间的拓扑结构.

\subsection{级联相关网络}
% ### 5.5.4 级联相关网络

机构自适应网络将网络结构也作为学习的目标之一,级联相关(Cascade-Correlation)网络是结构自适应网络的重要代表.

\subsection{Elman网络}
% ### 5.5.5 Elman网络

``递归神经网络''(recurrent neural networks)允许网络中出现环形结构,从而能处理与时间有关的动态变化.

\subsection{Boltzmann机}
% ### 5.5.6 Boltzmann机

Boltzmann机是一种``基于能量的模型''(energy-based model):它将网络状态定义一个``能量'',能量最小化时网络达到理想状态,而网络的训练就是在最小化这个能量函数.

Boltzmann机中的神经元都是布尔型的,状态$1$表示激活,状态$0$表示抑制.令向量$\mathbf s\in\{0,1\}^n$表示$n$个神经元的状态,$w_{ij}$表示神经元$i$与$j$之间的连接权,$\theta_i$表示神经元$i$的阈值,则状态$\mathbf s$所对应的Boltzmann机能量定义为
\begin{equation}
E(\mathbf s)=-\sum_{i=1}^{n-1}\sum_{j=i+1}^nw_{ij}s_is_j-\sum_{i=1}^n\theta_is_i\end{equation}

标准Boltzmann机是一个全连接图,难以解决实际任务,通常采用受限Boltzmann机解决实际问题.受限Boltzmann机仅保留了显层与隐层之间的连接,从而将Boltzmann机结构由完全图简化为二部图.

受限Boltzmann机常用``对比散度''(Contrastive Divergence)算法来进行训练.

\section{深度学习}
% ## 5.6 深度学习

多隐层神经网络难以使用经典算法进行训练,因为误差在多隐层内逆传播时,往往会发散而不能收敛到稳定状态.

无监督逐层训练(unsupervised layer-wise training)是多隐层网络训练的有效手段,其基本思想是每次训练一层隐结点,而本层隐结点的输出作为下一层隐结点的输入,这称为``预训练''(pre-training);在预训练全部完成后,再对整个网络进行``微调''(fine-tuning)训练.

另一种节省训练开销的策略是``权共享''(weight sharing),即让一组神经元使用相同的连接权.