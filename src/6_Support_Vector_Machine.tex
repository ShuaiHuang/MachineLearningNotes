\chapter{支持向量机}
% # 【《机器学习》第6章】支持向量机

本章的内容从逻辑上来说就是支持向量机的推导过程,其中涉及部分关于最优化理论的知识点,需要掌握:拉格朗日乘子法,对偶问题,KKT条件,松弛变量(见附录B).

\section{间隔与支持向量}
% ## 6.1 间隔与支持向量

给定训练样本集$D=\{(\bm x_1, y_1),(\bm x_2, y_2),\cdots,(\bm x_m, y_m)\},y_i\in\{-1,+1\}$,从直观上看,应该去找位于两类样本间的超平面去划分两类样本,而位于两类样本``正中间''的划分超平面泛化性能最好\footnote{首先要保证特征空间选取合适，即正负样本间要存在分类间隔.}.这就是支持向量机的最核心思想.

空间中超平面可以通过
\begin{equation}
    \bm {w}^T\bm{x}+b=0
\end{equation}
来描述,其中$\mathbf w=(w_1;w_2;\cdots,w_d)$为法向量;$b$为位移项;支持向量机的目标就是为了寻找参数$(\mathbf w,b)$.

假设超平面$(\mathbf w,b)$能将训练样本正确分类,即对于$(\bm x_i,y_i)\in D$,若$y_i=+1$则有$\bm {w^Tx_i}+b>0$;若$y_i=-1$则有$\bm{w^Tx_i}+b<0$.令
\begin{equation}
\left\{
\begin{array}{ll}
\bm{w^Tx_i}+b\ge +1,y_i=+1 \\
\bm{w^Tx_i}+b\le -1,y_i=-1
\end{array}
\right.
\end{equation}
距离超平面最近的样本点(即支持向量)使得上式中等号成立.两个异类支持向量到超平面的距离值和为
\begin{equation}
\gamma=\frac{2}{\|\bm w\|}
\end{equation}
$\gamma$也被称为间隔.

因此SVM基本型可以被定义为
\begin{equation}\begin{split}
\max_{\bm x,b}&\frac{2}{\|\bm w\|} \\
\text{\text{s.t.}}& \,y_i(\bm{w_i^Tx_i}+b)\ge 1,\quad i=1,2,\cdots,m.
\end{split}\end{equation}

上述问题等价为
\begin{equation}\begin{split}
\min_{\bm x,b}&\frac{1}{2}\|\bm w\|^2 \\
\text{s.t.}& \,y_i(\bm{w_i^Tx_i}+b)\ge 1,\quad i=1,2,\cdots,m.
\end{split}\end{equation}
这就是SVM的基本型.

\section{对偶问题}
% ## 6.2 对偶问题

SVM基本型本身一个凸二次规划问题,可以用现成的优化计算包求解.但是还有效率提升的空间.基本思路就是\textbf{通过拉格朗日乘子法求得其``对偶问题''(dual problem)}\footnote{为什么可以这样做？见附录B.1,在推导拉对偶问题时,常通过将拉格朗日乘子$L(\bm{x,\lambda,\mu})$对$\bm x$求导并令导数为$0$,来获得对偶函数的表达形式.值得注意的是,在强对偶性成立时(即主问题为凸优化问题),将拉格朗日函数分别对原变量和对偶变量求导,再并令导数等于$0$,即可得到原变量与对偶变量的数值关系.}.

该问题的拉格朗日函数可以写为
\begin{equation}L(\bm w, b, \bm\alpha)=\frac{1}{2}\|\bm w\|^2+\sum_{i=1}^m\alpha_i(1-y_i(\mathbf{w^Tx_i+b}))\end{equation}
令$L$对$\bm w$和$b$的偏导为$0$,并代回拉格朗日函数消去$\bm w$和$b$,得到对偶问题
\begin{equation}\begin{split}
\max_{\bm\alpha}&\sum_{i=1}^m\alpha_i-\frac{1}{2}\sum_{i=1}^m\sum_{j=1}^m\alpha_i\alpha_jy_iy_j\bm{x_i^Tx_j}\\
\text{s.t.}&\,\sum_{i=1}^m\alpha_iy_i=0,\\
&\alpha_i\ge 0,\quad i=1,2,\cdots,m.
\end{split}\end{equation}

解出$\bm\alpha$后,求出$\bm w$与$b$即可得到模型
\begin{equation}\begin{split}
f(\bm x)&=\bm{w^Tx}+b\\
&=\sum_{i=1}^m\alpha_iy_i\bm{x_i^Tx}+b
\end{split}\end{equation}

$\alpha_i$是拉格朗日乘子,它恰好对应着训练样本$(\bm x_i,y_i)$,注意到对偶问题中有不等式约束,因此上述推导过程需要满足KKT(Karush-Kuhn-Tucker)条件,
\begin{equation}\left\{\begin{array}{ll}
\alpha_i\ge 0\\
y_if(\bm x_i)-1\ge 0\\
\alpha_i(y_if(\bm x_i)-1)=0
\end{array}\right.\end{equation}
训练完成后,模型仅与支持向量有关.

上述问题可以通过SMO(Sequential Minimal Optimization)方法求解,其基本思路是固定除$\alpha_i$之外的所有参数,然后求$\alpha_i$上的极值,则$\alpha_i$可由其他变量导出.于是,SMO每次选择两个变量$\alpha_i$和$\alpha_j$,并固定其他参数.这样在参数初始化后,SMO不断执行如下两个步骤直至收敛:

\begin{itemize}
\item 选取一对需要更新的变量$\alpha_i$和$\alpha_j$;
\item 固定$\alpha_i$和$\alpha_j$以外的参数,求解对偶问题获得更新后的$\alpha_i$和$\alpha_j$.
\end{itemize}

\section{核函数}
% ## 6.3 核函数

前两节中,暗含有假设两类样本是线性可分的.但是在现实中,这样的条件很难被满足.对于这种情况,可以将样本从原始空间映射到一个更高维的特征空间,使得样本在这个特征空间内线性可分.

原问题可以推广为
\begin{equation}\begin{split}
\max_{\bm\alpha}&\sum_{i=1}^m\alpha_i-\frac{1}{2}\sum_{i=1}^m\sum_{j=1}^m\alpha_i\alpha_jy_iy_j\phi(\bm{x_i})^T\phi(\bm{x_j})\\
\text{s.t.}&\,\sum_{i=1}^m\alpha_iy_i=0,\\
&\alpha_i\ge 0,\quad i=1,2,\cdots,m.
\end{split}\end{equation}
上式涉及到计算$\phi(\bm{x_i})^T\phi(\bm{x_j})$,由于特征空间的维数很高,因此直接计算很困难.可以通过设置
\begin{equation}\kappa(\bm{x_i,x_j})=<\phi(\bm{x_i}),\phi(\bm{x_j})>=\phi(\bm{x_i})^T\phi(\bm{x_j})\end{equation}
只需要$\bm x_i$与$\bm x_j$在原始空间中计算函数$\kappa(\cdot,\cdot)$就可以得到在特征空间中的内积.

因此可得
\begin{equation}\begin{split}
f(\bm x)&=\bm w^T+b\\
&=\sum_{i=1}^m\alpha_iy_i\phi(\bm x_i)^T\phi(\bm x)+b\\
&=\sum_{i=1}^m\alpha_iy_i\kappa(\bm x,\bm x_i)+b.
\end{split}\end{equation}
上式显示出模型最优解可以通过训练样本的核函数展开,这一展式又被称为``支持向量展式''(support vector expansion).

一些核函数性质可见P128-P129.

\section{软间隔与正则化}
% ## 6.4 软间隔与正则化

另外一种解决训练样本线性不可分的方法是``软间隔''(soft margin),即允许支持向量机在一些样本上出错.因此,优化目标可以总结为
\begin{equation}\begin{split}
\min_{\bm w,b}\frac{1}{2}\|w\|^2+C\sum_{i=1}^ml_{0/1}(y_i(\bm{w^Tx_i}+b)-1)
\end{split}\end{equation}
其中$C>0$是一个常数,$l_{0/1}$是``0/1''损失函数
\begin{equation}l_{0/1}=\left\{\begin{array}{ll}
1,\,\text{if}\quad z<0\\
0,otherwise
\end{array}\right.\end{equation}

常见的损失函数有hinge损失,指数损失,对率损失(见P130).

采用hinge损失,引入``松弛变量''(slack variables),上式可重写为
\begin{equation}\begin{split}
\min_{\bm w,b,\xi}&\frac{1}{2}\|\bm w\|^2+C\sum_{i=1}^m\xi_i\\
\text{s.t.}&\,y_i(\bm{w^Tx_i}+b)\ge1-\xi_i\\
&\xi_i\ge0,\,i=1,2,\cdots,m.
\end{split}\end{equation}

对偶问题推导以及解稀疏性证明见P132.

\section{支持向量回归}
% ## 6.5 支持向量回归

支持向量引申到回归问题上即可得到支持向量回归(Support Vector Regression, SVR).SVR假设我们能容忍$f(\bm x)$与$y$之间最多有$\epsilon$的偏差,即仅当$f(\bm x)$与$y$之间的差别绝对值大于$\epsilon$时才计算损失.SVR问题可化为
\begin{equation}\begin{split}
\min_{\bm w,b}\frac{1}{2}\|\bm w\|^2+C\sum_{i=1}^ml_\epsilon(f(\bm x_i)-y_i)
\end{split}\end{equation}
其中$C$为正则化常数,$l_\epsilon$为$\epsilon$-不敏感损失函数
\begin{equation}l_\epsilon=\left\{\begin{array}{ll}
0,\quad\text{if}|z|\le\epsilon\\
|z|-\epsilon,otherwise
\end{array}\right.\end{equation}

对偶问题推导以及KKT条件证明见P135-137.

\section{核方法}
% ## 6.6 核方法

给定训练样本,不考虑偏移项$b$,则无论SVM还是SVR,学得的模型总能表示成核函数$\kappa(\bm{x_i,x_j})$的线性组合.因此有更一般的``表示定理''(representer theorem).P137

一般人们通过``核化''(引入核函数)来将线性学习器拓展为非线性学习器.P137-139即以线性判别分析为例,将其进行非线性拓展,得到``核线性判别分析''(Kernelized Linear Discriminant Analysis).
