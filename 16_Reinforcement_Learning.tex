\chapter{强化学习}
% # 【《机器学习》第16章】强化学习(Reinforcement Learning)

\section{任务与奖赏}
% ## 16.1 任务与奖赏

在对机器做操作的过程中,仅仅能通过操作获得当前反馈,而难以判断最终奖赏的结果,对这个过程进行抽象,便得到了"强化学习"的模型.强化学习任务通常用马尔可夫决策过程(Markov Decision Process, MDP)来描述:机器处于环境$E$中,状态空间为$X$,每个状态$x\in X$为机器对当前环境的描述;机器所采取的动作空间为$A$,每个动作$a\in A$作用在当前状态$x$上,使得状态以概率$P$转移到另一个状态,环境根据"奖赏"(reward)函数$R$反馈给机器一个奖赏.综上,强化学习对应四元组$E=<X,A,P,R>$.需要注意的是,在环境中状态的转移,奖赏的返回是不受机器控制的,机器只能通过选择要执行的动作来影响环境,也只能通过观察转移后的状态和返回的奖赏来感知环境.

机器要做的事情就是在环境中不断尝试而学得一个"策略"(policy)$\pi$,根据这个策略,在状态$x$下就能得知要执行的动作$a=\pi(x)$.一种策略表示是函数$\pi:X\mapsto A$;另一种策略表示是$\pi:X\times A\mapsto \mathbb R$.策略的优劣取决于长期执行这一策略后得到的累积奖赏.在强化学习任务中,学习的目的就是要找到能够使长期累积奖赏最大化的策略.

强化学习与有监督学习不同之处在于,在强化学习中并没有监督学习中的有标记样本,只有在学习过程最后得到最终结果.因此,强化学习在一定程度上可以看做"延迟标记信息"的监督学习问题.

\section{$K$-摇臂赌博机}
% ## 16.2 $K$-摇臂赌博机

\subsection{探索与利用}
% ### 16.2.1 探索与利用

欲最大化单步奖赏需要考虑两个方面:一是需要知道每个动作带来的奖赏,二是要执行奖赏最大的动作.

对于单步强化学习任务对应的"$K$-摇臂赌博机"(K-armed bandit),有两种策略:"仅探索"(expliration-only)法,将所有的尝试机会平均分给每个摇臂,最后以每个摇臂各自的平均吐币概率作为其奖赏期望的近似估计."仅利用"(exploitation-only)法,按下目前最优的摇臂,若有多个摇臂同为最优,则从中随机选取一个.

"探索"和"利用"这两者是互相矛盾的,这就是强化学习所面临的"探索-利用窘境"(Exploration-Exploitation dilemma).欲使得累计奖赏最大,必须在探索与利用之间达成较好的折中.

\subsection{$\epsilon$-贪心}
% ### 16.2.2 $\epsilon$-贪心

$\epsilon$-贪心基于一个概率来对探索和利用进行折中:每次尝试时,以$\epsilon$的概率进行探索,以$1-\epsilon$的概率进行利用.

\subsection{Softmax}
% ### 16.2.3 Softmax

Softmax算法基于当前已知的摇臂平均奖赏来对探索和利用进行折中.若各摇臂的平均奖赏相当,则选取各摇臂的概率也相当;若某些摇臂的奖赏明显高于其他摇臂,则它们被选取的概率也明显要高.概率分配是基于Boltzmann分布:
$$P(k)=\frac{e^\frac{Q(k)}{\tau}}{\sum_{i=1}^Ke^\frac{Q(i)}{\tau}}$$
其中,$Q(i)$记录当前摇臂的平均奖赏;$\tau>0$称为"温度",$\tau$越小则平均奖赏高的摇臂被选取的概率越高.

上述的$\epsilon$-贪心和Softmax算法都是针对单步强化学习问题的解决方案.如果简单地将多步强化学习看做多个单步强化学习的叠加,则会有很多局限性.因为它没有考虑强化学习任务中马尔可夫决策过程的结构.

\section{有模型学习}
% ## 16.3 有模型学习

如果任务对应的马尔科夫决策过程四元组$E=<X,A,P,R>$均已知，则称为“模型已知”,模型已知环境中的学习称为“有模型学习”（model-based learning）。

\subsection{策略评估}
% ### 16.3.1 策略评估

状态值函数（state value function）$V(\cdot)$评估的是从状态值$x$出发，使用策略$\pi$所带来的累积奖赏。

状态-动作值函数(state-action value function)$Q(\cdot)$评估的是从状态值$x$出发，执行动作$a$后再使用策略$\pi$所带来的累积奖赏。

由于MDP具有马尔可夫性质，即系统下一时刻的状态仅由当前时刻的状态决定，于是值函数有很简单的递归形式。

\subsection{策略改进}
% ### 16.3.2 策略改进

对策略进行评估的最终目的是为了判定其是否是最优策略，如果不是最优策略则需要对策略进行改进。理想的策略应该能最大化累积奖赏。
\begin{equation}
\pi^\ast={\arg\max}_\pi\sum_{x\in X}V^\pi(x)
\end{equation}

一个强化学习任务可能有多个最优策略，最优策略对应的值函数$V^\ast$称为最优值函数，即
\begin{equation}
\forall x\in X：V^\ast(x)=V^{\pi^\ast}(x)
\end{equation}
当策略空间无约束时$V^\ast$才是最优策略对应的值函数。如果策略空间有约束，则违背约束的策略是不合法的。

如果当前动作不是最优策略，则改进方式为：将策略选择的动作改变为当前最优的动作。

\subsection{策略迭代与值迭代}
% ### 16.3.3 策略迭代与值迭代

求解最优解的方法：从一个初始策略出发，先进行策略评估，然后改进策略，评估改进的策略，再进一步改进策略……不断迭代改进和评估，直到算法收敛为止。

策略迭代在每次改进策略后都需要重新进行策略评估，这比较耗费时间。

\section{免模型学习}
% ## 16.4 免模型学习

在现实的学习任务中，环境的转移概率、奖赏函数往往很难得知，如果算法不依赖于环境建模，则称为“免模型学习”（model-free learning)。

\subsection{蒙特卡罗强化学习}
% ### 16.4.1 蒙特卡罗强化学习

在免模型学习情形下，最大的问题就是策略无法评估。此时，只能通过在环境中执行不同的动作，来观察转移的状态和得到的奖赏。一种直接的策略评估替代方法是多次“采样”，然后求取平均累积奖赏来作为期望累积奖赏的近似，这称为蒙特卡罗强化学习。

策略迭代算法估计的是状态值函数$V$,而最终的策略是通过状态-动作值函数$Q$来获得。于是，需要将估计对象从$V$转变为$Q$，即估计每一对“状态-动作”的值函数。

由于在免模型学习情况下，只能从一个起始状态开始探索环境，因此只能在探索的过程中逐渐发现各个状态并估计各状态-动作对的值函数。

如果较好地获得值函数的估计，就需要多条不同的采样轨迹。获取不同采样轨迹的方法为（以$\epsilon-$贪心算法为例）：以$\epsilon$的概率从所有动作中均匀随机选取一个，以$1-\epsilon$概率选取当前最优动作。

如果被评估的策略与被改进的策略是同一个策略，则称之为“同策略”（on-policy）蒙特卡罗强化学习算法。然而，引入$\epsilon-$贪心算法是为了便于策略评估，实际上我们需要改进的策略是原始策略。为达到这一目的，可以借鉴以下思想：

一般的，函数$f$在概率分布$p$下的期望
\begin{equation}
\mathbb E[f]=\int_xp(x)f(x)dx
\end{equation}
可通过从概率分布$p$上的采样$\{x_1,x_2,\dots,x_m\}$来估计$f$的期望，即
\begin{equation}
\mathbb {\hat E}[f]=\frac{1}{m}\sum_{i=1}^mf(x_i)
\end{equation}
若引入另一个分布$q$，则函数$f$在概率分布$p$下的期望也可等价地写为
\begin{equation}
\mathbb E=\int_x q(x)\frac{p(x)}{q(x)}f(x)dx
\end{equation}
上式可以通过在$q$上的采样$\{x_1',x_2',\dots,x_m'\}$估计为
\begin{equation}
\mathbb {\hat E}[f]=\frac{1}{m}\sum_{i=1}^m\frac{p(x_i')}{q(x_i')}f(x_i')
\end{equation}
.

同样的，将上述分布$p,q$替换为对应的策略，就可以得到“异策略”（off-policy）蒙特卡罗强化学习算法。（见P386）

\subsection{时序差分学习}
% ### 16.4.2 时序差分学习

蒙特卡罗强化学习算法通过考虑采样轨迹，克服了模型未知给策略估计造成的困难。但是蒙特卡洛学习算法没有充分利用强化学习任务的MDP结构。时序差分（Temporal Difference, TD）学习则结合了动态规划与蒙特卡罗方法的思想，能做到更搞笑的学习。其本质就是在于将状态动作对的更新变成增量式的方式进行。（见P387）

典型代表算法有Sarsa算法，$Q$-学习算法。

\section{值函数近似}
% ## 16.5 值函数近似

前面所述的算法都是针对有限状态空间，现实的强化学习任务所面临的状态空间往往是连续的，有无穷多个状态。一个直接的想法就是对状态空间进行离散化，将连续状态空间转化为有限离散状态空间，但是这仍然是一个很难的问题。

事实上，不妨对连续状态空间的值函数进行学习。先**假定状态空间为$n$维实数空间$X=\mathbb R^n$,并且值函数能表达为状态的线性函数**
\begin{equation}
V_{\mathbf \theta}(\mathbf x)=\mathbf\theta^T\mathbf x
\end{equation}
其中$\mathbf x$为状态向量，$\theta$为参数向量。由于此时的值函数难以像有限状态那样精确记录每个状态的值，因此这样值函数的求解被称为值函数近似（value function approximation）。

\section{模仿学习}
% ## 16.6 模仿学习

强化学习的经典任务设置中，机器所能获得的反馈信息仅有多步决策后的累积奖赏。如果在决策过程中参考决策范例进行决策的方式称为“模仿学习”（imitation learning）。

\subsection{直接模仿学习}
% ### 16.6.1 直接模仿学习

直接模仿学习通过将状态动作轨迹上的状态动作对抽取出来，构造出一个新的数据集合，然后使用分类或者回归算法学习得到新的模型，这种方式称为直接模仿学习。

\subsection{逆模仿学习}
% ### 16.6.2 逆模仿学习

通常设计奖赏函数非常困难，从人类专家提供的范例数据中反推出奖赏函数有助于解决该问题，这就是“逆强化学习”（inverse reinforcement learning）。

逆强化学习的基本思想是：欲使机器做出与范例一致的行为，等价于在某个奖赏函数的环境中求解最优策略，该最优策略所产生的轨迹与范例数据一致。