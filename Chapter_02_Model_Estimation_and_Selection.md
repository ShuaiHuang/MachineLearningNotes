# 【《机器学习》第2章】模型评估与选择

## 2.1 经验误差与过拟合

"训练误差"(training error)或"经验误差"(empirical error)是指学习器在训练集上的误差;"泛化误差"(generalization error)是指学习器在新样本上的误差.

过拟合是机器学习面临的关键障碍,是无法避免的.从另一个角度来看,机器学习问题是NP难的,而有效的机器学习算法必然是在多项式时间内运行完成.若可避免过拟合,则通过经验误差最小化就能获得最优解,这就意味这我们构造性的证明了$P=NP$,因此,我们只要相信$P\ne NP$,过拟合就不可避免.

对于一个问题,往往有不同的模型可供选择,理想的解决方案是对候选模型的泛化误差进行评估,然后选择泛化误差最小的那个模型.

## 2.2 评估方法

泛化误差很难直接度量得到,通常使用在测试集上的测试误差作为泛化误差的近似.对样本集划分出测试集的方法如下:

### 2.2.1 留出法

留出法将数据集划分为两个互斥的集合,其中一个集合作为训练集,另外一个作为测试集.

需要注意的是,训练集和测试集的划分要尽可能保持数据分布的一致性.另外,一般采用若干次随机划分,重复进行实验评估后取平均值作为留出法的评估结果.

### 2.2.2 交叉验证法

交叉验证法将数据集划分为$k$个大小相似的互斥子集,并且保持每个子集上的数据一致性.进行$k$轮迭代,每次用$k-1$个子集的并集作为训练集,剩余的1个子集作为测试集.

如果$k$取值与训练集样本数目相等,则称为"留一法"(Leave-One-Out, LOO).但是留一法未必永远比其他模型准确,NFL定理对实验评估方法同样适用.

### 2.2.3 自助法

前面所述的留一法受训练样本规模影响较小,但是计算复杂度较高;其他方法会受到样本规模不同的影响."自助法"(boostrapping)是一种既不受样本规模影响又能高效进行实验估计的方法.

自助法通过对样本进行有放回采样,对数据集进行划分.但是这样会改变原始的样本分布,会引入估计偏差.在样本数量较多时,一般使用留出法和交叉验证法多一些.

### 2.2.4 调参与最终模型

一般模型是在训练集上进行训练,测试集上进行测试.如果通过训练过程选取到了最优的参数,则应将训练出的模型放到全部数据集上进行训练,得到最终的模型.

通常我们把学得模型在实际使用过程中遇到的数据集称为测试数据集,模型评估与选择中用于评估测试的数据集称为验证集.也就是说,测试集的功能是估计不同算法模型的泛化能力;验证集的作用是调参和模型选择.

## 2.3 性能度量

性能度量的目的是对模型的泛化能力有一个评价标准.在对比不同模型的能力时,使用不同的性能度量往往会导致不同的评判结果.

回归任务最常用的性能度量是"均方误差"(mean square error)
$$E(f;D)=\frac{1}{m}\sum_{i=1}^m(f(\mathbf x_i)-y_i)^2$$
更一般的情形下,对于数据分布$\mathcal D$和概率密度函数$p(\cdot)$,均方误差可描述为
$$E(f;D)=\int_{\mathbf x\sim\mathcal D}(f(\mathbf x)-y)^2p(\mathbf x)d\mathbf x$$

### 2.3.1 错误率与精度

错误率和精度是分类任务中最常用的两种性能度量.错误率是分类错误的样本数占样本总数的比例,精度是分类正确的样本数占样本总数的比例.

### 2.3.2 查准率,查全率与$F1$

混淆矩阵包含真正例(true positive),假正例(false positive),真反例(true negative),假反例(false positive),分别对应为$TP,FP,TN,FP$.

查准率$P$与查全率$R$分别定义为
$$P=\frac{TP}{TP+FP}$$
$$R=\frac{TP}{TP+FN}$$

查准率和查全率是一对磨盾的度量.

如果根据学习期的预测结果对样例进行排序,按此顺序逐个把样本作为正例进行预测,可以计算出当前的查全率,查准率.以查准率为纵轴,以查全率作为横轴,就得到了查准率-查全率曲线,简称P-R曲线.

笼统地说,如果一条曲线A能完全包住曲线B,则称A对应的学习器性能优于B对应的学习器.但是如果两条曲线有交叉,则很难断定那条曲线对应的学习器性能较好.因此,有必要对P-R曲线进行量化评判.

"平衡点"(Break-Event Point, BEP)度量的是$查准率=查全率$时的取值.

更常用的是$F1$度量
$$F1=\frac{2\times P\times R}{P+R}$$
如果对查全率和查准率的关注程度不同,可用
$$F_\beta=\frac{(1+\beta^2)\times P\times R}{(\beta^2\times P)+R}$$
其中,$\beta>0$度量了查全率对查准率的相对重要性;$\beta>1$时查全率有更大影响;$\beta<1$时查准率有更大影响.

如果有多个二分类混淆矩阵,可以分别计算每个混淆矩阵上的查全率与查准率,然后就可得到对应的宏查准率,宏查全率,宏$F1$
$$marco-P=\frac{1}{n}\sum_{i=1}^nP_i$$
$$marco-R=\frac{1}{n}\sum_{i=1}^nR_i$$
$$marco-F1=\frac{2\times marco-P \times marco-R}{marco-P+marco-R}$$

如果对这些混淆矩阵元素分别求平均值,得到$\overline{TP},\overline{FP},\overline{TN},\overline{FN}$,可得到微查准率,微查全率,微$F1$
$$micro-P=\frac{\overline{TP}}{\overline{TP}+\overline{FP}}$$
$$micro-R=\frac{\overline{TP}}{\overline{TP}+\overline{FN}}$$
$$micro-F1=\frac{2\times micro-P \times micro-R}{micro-P+micro-R}$$

### 2.3.3 ROC与AUC

ROC曲线的纵轴是"真正例率"(True Positive Rate, TPR),横轴是"假正例率"(False Positive Rate),分别定义为
$$TPR=\frac{TP}{TP+FN}$$
$$FPR=\frac{FP}{TN+FP}$$

一般比较合理的通过ROC度量学习器性能的方式为比较ROC曲线下的面积,即AUC(Area Under ROC Curve).

### 2.3.4 代价敏感错误率与代价曲线

现实任务中,不同的错误类型所造成的后果不同,可以认为是"非均等代价"(unequal cost).前面的几种度量方式都隐式地假设了均等代价.可通过"代价矩阵"(cost matrix)对代价度量进行扩充.(见P35)

在非均等代价下,ROC曲线不能直接反映出学习器的期望总体代价,而代价曲线(cost curve)则可达到该目的.

ROC曲线上的每一点对应了代价平面上的一条线段.设ROC曲线上一点坐标$(TPR,FPR)$,则计算出FNR,然后代价平面上有一条从$(0,FPR)$到$(1,FNR)$的线段.所有线段围成的面积即为在所有条件下学习器的期望总体代价.

## 2.4 比较检验

性能度量还存在几个重要问题:

1. 我们希望比较的是泛化性能,不是测试集上的误差
2. 测试集上的性能与测试集本身有很大关系
3. 机器学习的算法具有随机性

统计假设检验为我们进行学习器性能比较提供了重要依据.各种假设检验见P37-P44.

## 2.5 偏差与方差

"偏差-方差分解"(bias-variance decomposition)是解释学习器泛化性能的一种重要工具.偏差-方差分解试图对学习算法的期望泛化错误率进行拆解.拆解过程见P45推导过程,最终结果为
$$E(f;D)=bias^2(\mathbf x)+var(\mathbf x)+\varepsilon^2$$
即泛化误差可以分解为偏差,方差和噪声之和.

偏差度量了学习算法的期望预测与真实结果的偏离程度;方差度量了同样大小的训练集的变动所导致的学习器性能的变化;噪声表达了在当前任务上任何学习算法所能达到的期望泛化误差的下界.

一般来说,偏差方差是有冲突的,一般称为"偏差-方差窘境"(bias-variance dilemma).训练不足时,训练数据的扰动不足以使学习器产生显著变化;训练充足时,学习器拟合能力非常强,训练数据的轻微扰动都会使学习器产生显著变化.